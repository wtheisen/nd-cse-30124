{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e53f42e",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Intelligence: Practice Packet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8023f",
   "metadata": {},
   "source": [
    "Welcome to CSE 30124 - Introduction to Artificial Intelligence\n",
    "\n",
    "This packet is intended to be a simple review of material I will expect familiarity with (due to prereqs!) and a brief introduction to some of the libraries we will be using throughout the course. This packet is not comprehensive, but completing the packet will certainly help you prepare for class.\n",
    "\n",
    "## Table of Contents\n",
    "1. **Introduction: Jupyter Notebooks**\n",
    "\n",
    "3. **Review: Linear Transformations**\n",
    "\n",
    "4. **Introduction: Probability Distributions**\n",
    "5. **Introduction: Data Distributions**\n",
    "6. **Introduction: Function Approximation**\n",
    "7. **Introduction: Python Libraries for AI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06a8e7",
   "metadata": {},
   "source": [
    "## 1. Introduction to Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e54372",
   "metadata": {},
   "source": [
    "\n",
    "Jupyter Notebooks allow you to write and execute Python code in an interactive environment. \n",
    "They consist of cells that can either contain text (Markdown) or code.\n",
    "\n",
    "### Key Features:\n",
    "- **Code Cells**: Run Python code interactively.\n",
    "- **Markdown Cells**: Write text, equations, and explanations using Markdown.\n",
    "- **Interactivity**: Update code and text dynamically.\n",
    "\n",
    "Try running the code below by selecting the cell and pressing `Shift + Enter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello, CSE 30124')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276e517",
   "metadata": {},
   "source": [
    "### Markdown Cells\n",
    "Markdown cells are used to write and format text. You can use Markdown to add headings, lists, links, images, and even LaTeX equations to your notebook.\n",
    "\n",
    "Here's an example of a Markdown list:\n",
    "\n",
    "- **Bold text**: `**Bold text**`\n",
    "- *Italic text*: `*Italic text*`\n",
    "- [Link](https://www.example.com): `[Link](https://www.example.com)`\n",
    "\n",
    "You can also add images using Markdown:\n",
    "\n",
    "`![Image description](https://www.example.com/image.png)`\n",
    "\n",
    "Try editing this cell and adding some of your own Markdown elements!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5ec46",
   "metadata": {},
   "source": [
    "### Code\n",
    "Now that you know the basics of Markdown and Code cells, it's time for you to create your own pieces of a notebook\n",
    "\n",
    "1. Create a new Markdown cell below this one. In that cell, write a brief paragraph about what you have used AI for in the past and a mistake you found that it made.\n",
    "2. Create a new Code cell below your Markdown cell. In that cell, write a Python function that takes a number as input and returns the square of that number.\n",
    "3. Run your code cell to make sure your function works correctly.\n",
    "\n",
    "Good luck, and welcome to CSE 30124!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f03d5",
   "metadata": {},
   "source": [
    "## Key Concepts:\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra that can be interpreted as a linear transformation.\n",
    "\n",
    "- **Linear Transformation**: A function between two vector spaces that preserves vector addition and scalar multiplication.\n",
    "- **Matrix Representation**: Any linear transformation can be represented as a matrix.\n",
    "- **Matrix Multiplication**: Applying a matrix to a vector transforms the vector according to the linear transformation represented by the matrix.\n",
    "\n",
    "## Introduction to Vector Spaces and Subspaces\n",
    "\n",
    "In mathematics, particularly in linear algebra, the concepts of vector spaces and subspaces are foundational for understanding the structure and behavior of vectors.\n",
    "\n",
    "### Vector Space:\n",
    "A **vector space** is a collection of vectors that can be added together and multiplied by scalars (real numbers), satisfying certain axioms. These axioms include associativity, commutativity of addition, existence of an additive identity (zero vector), and distributive properties of scalar multiplication.\n",
    "\n",
    "#### Real-World Example:\n",
    "Consider the set of all possible positions of a drone in a 3D space. Each position can be represented as a vector \\((x, y, z)\\), where \\(x\\), \\(y\\), and \\(z\\) are coordinates in space. The set of all such vectors forms a vector space because you can add two position vectors or scale a position vector by a real number to get another valid position vector.\n",
    "\n",
    "### Subspace:\n",
    "A **subspace** is a subset of a vector space that is itself a vector space under the same operations of addition and scalar multiplication. For a subset to be a subspace, it must include the zero vector, be closed under vector addition, and be closed under scalar multiplication.\n",
    "\n",
    "#### Real-World Example:\n",
    "Continuing with the drone example, imagine the drone is restricted to fly only at a constant altitude, say \\(z = 10\\). The set of all position vectors \\((x, y, 10)\\) forms a subspace of the original 3D vector space. This subspace is essentially a 2D plane within the 3D space, where the drone can move freely in the \\(x\\) and \\(y\\) directions but not in the \\(z\\) direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e92a38",
   "metadata": {},
   "source": [
    "### High Dimensional Data\n",
    "\n",
    "While it's easy to visualize a drone in 3D space you may be wondering why this matters. In AI, we often aren't operating on data that is in a \"real\" space. Instead our data is highly abstract.\n",
    "\n",
    "A classic data set in Machine Learning is the [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). How would you describe a flower to a computer? Representation of data is one of the trickiest and most important things in AI.\n",
    "\n",
    "One way we could represent a flower in the computer is to give it four different measurements based on attributes of the flower. In the Iris dataset we have four measurements: Petal Length, Petal Width, Sepal Length, and Sepal Width.\n",
    "\n",
    "![Iris](https://miro.medium.com/v2/resize:fit:875/1*H2UmG5L1I5bzFCW006N5Ag.png)\n",
    "\n",
    "In this way we can describe each type of iris to the computer, in a way that lets us compare between the three types of irises in the data.\n",
    "\n",
    "![Iris Classes](https://miro.medium.com/v2/resize:fit:1400/1*f6KbPXwksAliMIsibFyGJw.png)\n",
    "\n",
    "More importantly however, we could imagine each Iris being a point in 4D space! Each axis of this space corresponds to one of our 4 measurements, and by combining all four measurements used to describe a single flower, we could plot a point in space to represent this flower! This also means that if we could somehow compute the distance between two points in 4D space we could tell how different or similar any two given flowers are.\n",
    "\n",
    "It's pretty hard to visualize 4D space but it's even harder to visualize 12,288 dimensional space, which is how many dimensions a word has in chatGPT 3. The important take away here is that regardless of what our data set is, we can usually represent it as a collection of points in space which allows us to use linear algebra to learn things about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a2104",
   "metadata": {},
   "source": [
    "### First Neural Network\n",
    "\n",
    "Run the code cell below to see your first neural network in action! We will dive into this example during multiple lectures this semester, but it is a nice visualization of the use of linear algebra to transform input data to allow us to make decisions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67321123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Generate synthetic 3D input data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 3)  # 3D input\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # Simple binary classification (based on a plane)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(3, 2)  # Reduce 3D to 2D\n",
    "        self.output = nn.Linear(2, 1)  # Reduce 2D to 1D\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.hidden(x)\n",
    "        out = self.output(hidden)\n",
    "        return hidden, out\n",
    "\n",
    "# Initialize the model, loss, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Metrics to track\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    hidden_out, output = model(X_tensor)\n",
    "    loss = criterion(output, y_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    with torch.no_grad():\n",
    "        predictions = torch.sigmoid(output) > 0.5  # Binary predictions\n",
    "        accuracy = (predictions == y_tensor).float().mean().item()\n",
    "    \n",
    "    # Track metrics\n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Convert outputs to NumPy for visualization\n",
    "    hidden_out_np = hidden_out.detach().numpy()\n",
    "    output_np = torch.sigmoid(output).detach().numpy()  # Apply sigmoid for probabilities\n",
    "\n",
    "    # Select the first sample to show real inputs and outputs\n",
    "    sample_idx = 0\n",
    "    x_sample = X[sample_idx]  # Real input\n",
    "    hidden_sample = hidden_out_np[sample_idx]  # Hidden layer output\n",
    "    output_sample = output_np[sample_idx][0]  # Final output\n",
    "\n",
    "    # Visualization setup\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    # Create 3D subplot for the input space\n",
    "    ax_3d = fig.add_subplot(231, projection='3d')\n",
    "    ax_2d = axs[0, 1]\n",
    "    ax_1d = axs[0, 2]\n",
    "\n",
    "    colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "    \n",
    "    # 3D space\n",
    "    ax_3d.scatter(X[:, 0], X[:, 1], X[:, 2], c=colors, alpha=0.7)\n",
    "    ax_3d.scatter(x_sample[0], x_sample[1], x_sample[2], c='yellow', s=100, edgecolor='black', label='Highlighted Sample')\n",
    "    ax_3d.set_title(f\"3D Input Space (Epoch {epoch})\")\n",
    "    ax_3d.set_xlabel(\"X1\")\n",
    "    ax_3d.set_ylabel(\"X2\")\n",
    "    ax_3d.set_zlabel(\"X3\")\n",
    "    ax_3d.legend()\n",
    "    \n",
    "    # 2D hidden space\n",
    "    ax_2d.scatter(hidden_out_np[:, 0], hidden_out_np[:, 1], c=colors, alpha=0.7)\n",
    "    ax_2d.scatter(hidden_sample[0], hidden_sample[1], c='yellow', s=100, edgecolor='black', label='Highlighted Sample')\n",
    "    ax_2d.set_title(\"2D Hidden Layer with Decision Boundary\")\n",
    "    ax_2d.set_xlabel(\"H1\")\n",
    "    ax_2d.set_ylabel(\"H2\")\n",
    "    ax_2d.legend()\n",
    "    \n",
    "    # 1D output space\n",
    "    ax_1d.scatter(output_np[:, 0], np.zeros_like(output_np[:, 0]), c=colors, alpha=0.7)\n",
    "    ax_1d.scatter(output_sample, 0, c='yellow', s=100, edgecolor='black', label='Highlighted Sample')\n",
    "    ax_1d.axvline(0.5, color='green', linestyle='--', label='Decision Boundary (0.5)')\n",
    "    ax_1d.set_title(\"1D Output Space with Decision Boundary\")\n",
    "    ax_1d.set_xlabel(\"Output\")\n",
    "    ax_1d.legend()\n",
    "    \n",
    "    # Learning curve plot\n",
    "    axs[1, 0].plot(range(1, epoch + 1), losses, label=\"Loss\", color='blue')\n",
    "    axs[1, 0].set_title(\"Learning Curve\")\n",
    "    axs[1, 0].set_xlabel(\"Epoch\")\n",
    "    axs[1, 0].set_ylabel(\"Loss\")\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axs[1, 1].plot(range(1, epoch + 1), accuracies, label=\"Accuracy\", color='green')\n",
    "    axs[1, 1].set_title(\"Accuracy Curve\")\n",
    "    axs[1, 1].set_xlabel(\"Epoch\")\n",
    "    axs[1, 1].set_ylabel(\"Accuracy\")\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    # Display weights, biases, and computation in LaTeX-style\n",
    "    axs[1, 2].axis('off')  # Turn off axis\n",
    "    \n",
    "    # Highlighted sample (we'll use the first sample for simplicity)\n",
    "    sample_idx = 0\n",
    "    x_sample = X[sample_idx]  # Real input for the highlighted sample\n",
    "    hidden_sample = hidden_out_np[sample_idx]  # Hidden layer output for the sample\n",
    "    output_sample = output_np[sample_idx][0]  # Final output for the sample\n",
    "\n",
    "    # Generate the weight matrices and biases as NumPy arrays for computation\n",
    "    hidden_weights = model.hidden.weight.detach().numpy()\n",
    "    hidden_biases = model.hidden.bias.detach().numpy()\n",
    "    output_weights = model.output.weight.detach().numpy()\n",
    "    output_bias = model.output.bias.detach().numpy()\n",
    "\n",
    "    # Perform manual computations for display\n",
    "    hidden_layer_result = hidden_weights @ x_sample + hidden_biases\n",
    "    output_layer_result = output_weights @ hidden_sample + output_bias\n",
    "\n",
    "    # Generate plain-text equations with better alignment and spacing\n",
    "    equation_text = (\n",
    "        f\"Hidden Layer:\\n\"\n",
    "        f\"  [{hidden_weights[0,0]:.2f}, {hidden_weights[0,1]:.2f}, {hidden_weights[0,2]:.2f}]   *   [{x_sample[0]:.2f}]\\n\"\n",
    "        f\"  [{hidden_weights[1,0]:.2f}, {hidden_weights[1,1]:.2f}, {hidden_weights[1,2]:.2f}]       [{x_sample[1]:.2f}]\\n\"\n",
    "        f\"                                      [{x_sample[2]:.2f}]\\n\"\n",
    "        f\"+ [{hidden_biases[0]:.2f}]\\n\"\n",
    "        f\"  [{hidden_biases[1]:.2f}]\\n\"\n",
    "        f\"= [{hidden_layer_result[0]:.2f}]\\n\"\n",
    "        f\"  [{hidden_layer_result[1]:.2f}]\\n\\n\"\n",
    "        f\"Output Layer:\\n\"\n",
    "        f\"  [{output_weights[0,0]:.2f}, {output_weights[0,1]:.2f}]   *   [{hidden_sample[0]:.2f}]\\n\"\n",
    "        f\"                                      [{hidden_sample[1]:.2f}]\\n\"\n",
    "        f\"+ [{output_bias[0]:.2f}]\\n\"\n",
    "        f\"= [{output_layer_result[0]:.2f}]\\n\\n\"\n",
    "        f\"Sigmoid Output: Sigma({output_layer_result[0]:.2f}) = {output_sample:.2f}\"\n",
    "    )\n",
    "    axs[1, 2].axis('off')  # Turn off the subplot axes\n",
    "    axs[1, 2].text(0.1, 0.5, equation_text, fontsize=10, verticalalignment='center', transform=axs[1, 2].transAxes)\n",
    "\n",
    "    # Update the visualization\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch {epoch}/{n_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Final static display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77cb593",
   "metadata": {},
   "source": [
    "Don't worry if you don't understand in either the code or the visualization, that's the point of this class! By the end of the semester this will be as easy as programming a fractal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a7ba6",
   "metadata": {},
   "source": [
    "## 3. Introduction: Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c67a6b",
   "metadata": {},
   "source": [
    "\n",
    "Probability distributions describe how probabilities are distributed over events.\n",
    "\n",
    "### Example: Tossing a Coin\n",
    "- **Uniform Distribution**: Equal probability for heads and tails (50% each).\n",
    "- **Normal Distribution**: A bell curve where values are more likely near the mean.\n",
    "\n",
    "### Practice Question:\n",
    "Simulate rolling a die 1000 times and plot the frequency of each outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e52026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate rolling a die\n",
    "rolls = np.random.randint(1, 7, size=1000)\n",
    "\n",
    "# Plot the frequency\n",
    "plt.hist(rolls, bins=np.arange(1, 8)-0.5, edgecolor='black')\n",
    "plt.title(\"Die Roll Frequencies\")\n",
    "plt.xlabel(\"Die Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(range(1, 7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6eb58",
   "metadata": {},
   "source": [
    "This is an example of a uniform distribution, where every event has an equally likely outcome. However a more common distribution is the normal, or gaussian, distribution. Consider the iris dataset above and try and imagine gathering 1,000,000 iris setosas. If you measured all of their petals and then plotted them what you'd likely discover is a normal distribution of petals. There is some sort of \"platonic\" petal length for a setosa and the flowers don't usually deviate that far from that ideal, average length of petal.\n",
    "\n",
    "![Petal Lengths](https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-09-9780128147610.jpg)\n",
    "\n",
    "Imagine you were given a new flower and you wanted to figure out which of the three types of iris it was. What's one thing you could try?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0303ca3",
   "metadata": {},
   "source": [
    "## 4. Introduction: Data Distributions\n",
    "\n",
    "The idea of data distributions is a very powerful one, but also very important to understand to help mitigate bias in artificial intelligence. \n",
    "\n",
    "Much like the idea of a probability distribution, a data distribution is a sort of description of the data you have. A question I asked on the final exam last semester was\n",
    "\n",
    "> A Norwegian company has a face identification tool that they trained on participants in yoga classes near their offices. They are about to roll out their model globally, do you think this model will work well?\n",
    "\n",
    "It's important here to consider the distribution of the training data used for the AI model. Was the training data distribution representative of the global distribution? In this question of course not.\n",
    "\n",
    "[Police Facial Recognition Technology Can’t Tell Black People Apart](https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/)\n",
    "\n",
    "This is a colossal issue AI faces, a model is only as good as the data you feed it. Data distrubtions and the representativness of the data you have access to are extremely important to keep in mind as you design models. You can never have all the training data to ever exist. You could never possibly measure every single iris to ever exist but hopefully you can collect a data set that is representative enough to model the underlying \"functions\" that generate irises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1ee68",
   "metadata": {},
   "source": [
    "## 5. Introduction: Function Approximation\n",
    "\n",
    "This leads into the idea of function approximation. Mostly what we do with machine learning is try and create a model by learning from our training data that approximates the hidden function that generated the training data.\n",
    "\n",
    "Take for example the relationship between caloric intake and blood sugar. This is an extremely complex relationship but there is some hidden function, known exactly only to God, that maps from caloric intake to blood sugar exactly. However we're only human so all we can do is collect a bunch of samples of caloric intake and blood sugar measurements. Imagine we then plotted these samples:\n",
    "\n",
    "![Caloric Intake to Blood Sugar](https://machinelearningmastery.com/wp-content/uploads/2021/07/approx1.png)\n",
    "\n",
    "What we can do is train a model to fit our dataset. If we only had the data points and were given a new caloric intake value we couldn't predict the expected blood sugar. By fitting a model to our data though we can try and approximate the hidden function and if our approximation is good we can mostly predict the expected blood sugar given a new caloric intake.\n",
    "\n",
    "Much of modern AI can be thought of as just a function approximation task.\n",
    "\n",
    "[Neural Networks are Function Approximation Algorithms](https://machinelearningmastery.com/neural-networks-are-function-approximators/)\n",
    "\n",
    "This article goes more in depth if you're interested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5264dda",
   "metadata": {},
   "source": [
    "## 6. Introduction: Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61463cae",
   "metadata": {},
   "source": [
    "# Essential Python Libraries for AI and Machine Learning\n",
    "\n",
    "Python has become the de facto language for AI and machine learning, largely due to its powerful ecosystem of specialized libraries. Let's explore the three most fundamental libraries you'll use in your AI journey.\n",
    "\n",
    "## NumPy: The Foundation of Numerical Computing\n",
    "\n",
    "NumPy (Numerical Python) is the fundamental package for scientific computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a vast collection of mathematical functions to operate on these arrays.\n",
    "The code block below provide a number of examples of basic numpy operations. I don't expect you to memorize these or anything but it would be good for you to be at least vaguely familiar with both the syntax and what's possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67582ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#always import it as np\n",
    "import numpy as np\n",
    "\n",
    "# Create a 1D array\n",
    "array_1d = np.array([1, 2, 3, 4])\n",
    "print(\"1D Array:\", array_1d)\n",
    "\n",
    "# Create a 2D array\n",
    "array_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"\\n2D Array:\\n\", array_2d)\n",
    "\n",
    "# Create arrays filled with zeros or ones\n",
    "zeros = np.zeros((2, 3))\n",
    "ones = np.ones((2, 3))\n",
    "print(\"\\nZeros:\\n\", zeros)\n",
    "print(\"\\nOnes:\\n\", ones)\n",
    "\n",
    "# Create an array with a range of numbers\n",
    "range_array = np.arange(0, 10, 2)  # start, stop, step\n",
    "print(\"\\nRange Array:\", range_array)\n",
    "\n",
    "# Create an array with evenly spaced values\n",
    "linspace_array = np.linspace(0, 1, 5)  # start, stop, number of points\n",
    "print(\"\\nLinspace Array:\", linspace_array)\n",
    "\n",
    "array_a = np.array([1, 2, 3])\n",
    "array_b = np.array([4, 5, 6])\n",
    "\n",
    "# Element-wise addition\n",
    "print(\"Addition:\", array_a + array_b)\n",
    "\n",
    "# Element-wise multiplication\n",
    "print(\"Multiplication:\", array_a * array_b)\n",
    "\n",
    "# Broadcasting: Adding a scalar to an array\n",
    "print(\"Add scalar:\", array_a + 10)\n",
    "\n",
    "# Element-wise square\n",
    "print(\"Square:\", array_a ** 2)\n",
    "\n",
    "array = np.array([10, 20, 30, 40, 50])\n",
    "\n",
    "# Indexing\n",
    "print(\"First element:\", array[0])\n",
    "\n",
    "# Slicing\n",
    "print(\"Slice (1:4):\", array[1:4])\n",
    "\n",
    "# Modifying elements\n",
    "array[0] = 99\n",
    "print(\"Modified Array:\", array)\n",
    "\n",
    "array = np.arange(1, 10)  # Array with values 1 to 9\n",
    "reshaped = array.reshape((3, 3))  # Reshape to 3x3\n",
    "print(\"Original Array:\", array)\n",
    "print(\"\\nReshaped Array:\\n\", reshaped)\n",
    "\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Sum of elements\n",
    "print(\"Sum:\", np.sum(array))\n",
    "\n",
    "# Mean and standard deviation\n",
    "print(\"Mean:\", np.mean(array))\n",
    "print(\"Standard Deviation:\", np.std(array))\n",
    "\n",
    "# Maximum and minimum\n",
    "print(\"Max:\", np.max(array))\n",
    "print(\"Min:\", np.min(array))\n",
    "\n",
    "# Random numbers between 0 and 1\n",
    "random_array = np.random.rand(3, 3)\n",
    "print(\"Random Array:\\n\", random_array)\n",
    "\n",
    "# Random integers\n",
    "random_ints = np.random.randint(0, 10, (2, 3))  # range [0, 10), shape (2, 3)\n",
    "print(\"\\nRandom Integers:\\n\", random_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7cf73",
   "metadata": {},
   "source": [
    "Numpy is significantly faster than basic python for mathematical, matrix, and vector operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47108971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate a large list and a NumPy array with the same values\n",
    "size = 10**6  # 1 million elements\n",
    "python_list = list(range(size))\n",
    "numpy_array = np.arange(size)\n",
    "\n",
    "# Pure Python: Compute the square of each element\n",
    "start_time = time.time()\n",
    "python_result = [x ** 2 for x in python_list]\n",
    "python_time = time.time() - start_time\n",
    "print(f\"Pure Python took {python_time:.5f} seconds\")\n",
    "\n",
    "# NumPy: Compute the square of each element\n",
    "start_time = time.time()\n",
    "numpy_result = numpy_array ** 2\n",
    "numpy_time = time.time() - start_time\n",
    "print(f\"NumPy took {numpy_time:.5f} seconds\")\n",
    "\n",
    "# Print the speedup\n",
    "speedup = python_time / numpy_time\n",
    "print(f\"NumPy is approximately {speedup:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ee2bf",
   "metadata": {},
   "source": [
    "### Pandas: Data Manipulation and Analysis\n",
    "Pandas provides high-performance, easy-to-use data structures and tools for working with structured data. It's particularly good at handling tabular data with heterogeneously-typed columns. It's sort of like a super advanced dictionary crossed with a spread sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from a dictionary\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Age\": [25, 30, 35, 40],\n",
    "    \"Score\": [85, 90, 95, 100],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame:\\n\", df)\n",
    "\n",
    "# Access a single column\n",
    "print(\"Names:\\n\", df[\"Name\"])\n",
    "\n",
    "# Add a new column\n",
    "df[\"Pass\"] = df[\"Score\"] >= 90\n",
    "print(\"\\nUpdated DataFrame:\\n\", df)\n",
    "\n",
    "# Filter rows where 'Age' is greater than 30\n",
    "filtered_df = df[df[\"Age\"] > 30]\n",
    "print(\"\\nFiltered Rows:\\n\", filtered_df)\n",
    "\n",
    "# Using a for loop to calculate grades\n",
    "grades = []\n",
    "for score in df[\"Score\"]:\n",
    "    if score >= 90:\n",
    "        grades.append(\"A\")\n",
    "    else:\n",
    "        grades.append(\"B\")\n",
    "df[\"Grade\"] = grades\n",
    "print(\"Grades with loop:\\n\", df)\n",
    "\n",
    "# Vectorized operation using pandas `apply` and lambda\n",
    "df[\"Grade\"] = df[\"Score\"].apply(lambda x: \"A\" if x >= 90 else \"B\")\n",
    "print(\"\\nGrades with Pandas:\\n\", df)\n",
    "\n",
    "# Group data by 'Grade' and calculate average age\n",
    "grouped = df.groupby(\"Grade\")[\"Age\"].mean()\n",
    "print(\"\\nAverage Age by Grade:\\n\", grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa1d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large DataFrame\n",
    "size = 10**6\n",
    "data = pd.DataFrame({\n",
    "    \"A\": np.random.rand(size),\n",
    "    \"B\": np.random.rand(size),\n",
    "})\n",
    "\n",
    "# Python loop: Add two columns\n",
    "start_time = time.time()\n",
    "data[\"C_loop\"] = [data[\"A\"][i] + data[\"B\"][i] for i in range(size)]\n",
    "python_time = time.time() - start_time\n",
    "print(f\"Python loop took {python_time:.5f} seconds\")\n",
    "\n",
    "# Pandas vectorized operation\n",
    "start_time = time.time()\n",
    "data[\"C_vectorized\"] = data[\"A\"] + data[\"B\"]\n",
    "pandas_time = time.time() - start_time\n",
    "print(f\"Pandas vectorized operation took {pandas_time:.5f} seconds\")\n",
    "\n",
    "# Speedup\n",
    "speedup = python_time / pandas_time\n",
    "print(f\"Pandas is approximately {speedup:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e8054",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "Scikit-learn is the most popular machine learning library in Python. It provides a consistent interface for a wide range of machine learning algorithms. We'll spend a lot more time looking at and using sklearn during Unit 02 but below is an example of using a model on the iris dataset to predict what type of iris a new flower is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf1bb",
   "metadata": {},
   "source": [
    "## E-Signature (5 pts.)\n",
    "\n",
    "Type your name here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
