{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearLayer:\n",
    "    \"\"\"\n",
    "    A fully connected (dense) layer that performs a linear transformation.\n",
    "\n",
    "    Attributes:\n",
    "        W (numpy.ndarray): Weight matrix with shape (output_dim, input_dim).\n",
    "        b (numpy.ndarray): Bias vector with shape (output_dim, 1).\n",
    "        X (numpy.ndarray): Cached input used during the forward pass.\n",
    "        dW (numpy.ndarray): Gradient with respect to the weights.\n",
    "        db (numpy.ndarray): Gradient with respect to the biases.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the LinearLayer with random weights and biases using He initialization.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features.\n",
    "            output_dim (int): Number of neurons (output features).\n",
    "\n",
    "        Weight initialization:\n",
    "            Weights and biases are initialized from a normal distribution and scaled by sqrt(2/input_dim).\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.random.randn(output_dim, 1) * np.sqrt(2.0 / input_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the linear layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (input_dim, m) where m is the number of examples.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Linear output with shape (output_dim, m)\n",
    "\n",
    "        Notes:\n",
    "            The input X is stored for use during backpropagation.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Store the input and calculate the output of the linear layer\n",
    "        pass\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Compute the backward pass of the linear layer.\n",
    "\n",
    "        Args:\n",
    "            dA (numpy.ndarray): Gradient of the loss with respect to the output of this layer,\n",
    "                                having shape (output_dim, m).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input X,\n",
    "                           with shape (input_dim, m).\n",
    "\n",
    "        Updates:\n",
    "            Sets self.dW as the gradient with respect to W (shape: (output_dim, input_dim)).\n",
    "            Sets self.db as the gradient with respect to b (shape: (output_dim, 1)).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the gradient of the loss with respect to the weights and biases\n",
    "        # TODO: Return the gradient of the loss with respect to the input\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update the parameters of the layer using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate for the parameter update.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Update the weights and biases of the layer using the learning rate\n",
    "        pass\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) activation function.\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass using ReLU activation.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data of any shape.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying ReLU element-wise (same shape as X).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Store the input and calculate the output of the ReLU layer\n",
    "        pass\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for the ReLU activation.\n",
    "\n",
    "        Args:\n",
    "            dA (numpy.ndarray): Gradient of the loss with respect to the ReLU output,\n",
    "                                having the same shape as the input X.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient of the loss with respect to the input X.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the gradient of the loss with respect to the input\n",
    "        # TODO: Return the gradient of the loss with respect to the input\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update function for ReLU activation. Since ReLU has no parameters, no update is performed.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # TODO: Update the weights and biases of the layer using the learning rate\n",
    "        pass\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function typically used at the output layer for multi-class classification.\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass using softmax activation.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (n_classes, m), where n_classes is the number of classes\n",
    "                               and m is the number of examples.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Softmax probabilities with shape (n_classes, m).\n",
    "        \"\"\"\n",
    "        # TODO: Store the input and calculate the output of the softmax layer\n",
    "        pass\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Compute the backward pass for the softmax activation.\n",
    "\n",
    "        Args:\n",
    "            dA (numpy.ndarray): Gradient of the loss with respect to the softmax output,\n",
    "                                having shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Passed-through gradient\n",
    "\n",
    "        Note:\n",
    "            Often the derivative is combined with cross-entropy loss simplifying the gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the gradient of the loss with respect to the input\n",
    "        # TODO: Return the gradient of the loss with respect to the input\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Update function for Softmax activation. No update is performed because softmax has no trainable parameters.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Update the weights and biases of the layer using the learning rate\n",
    "        pass\n",
    "\n",
    "class CleanNumpyNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network implemented using numpy for classification tasks on MNIST-like data.\n",
    "\n",
    "    Assumed Input:\n",
    "        - X: Each column is a flattened 28x28 MNIST style image, i.e., shape (784, m) where m is the number of examples.\n",
    "\n",
    "    Example Architecture:\n",
    "        - Layer 1: Linear layer mapping from 784 to 26 features.\n",
    "        - Output Activation: Softmax.\n",
    "\n",
    "    The network supports forward propagation, backpropagation (with cross-entropy loss derivative),\n",
    "    converting probabilities to class labels, and training via mini-batch gradient descent.\n",
    "    \"\"\"\n",
    "    def __init__(self, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize the neural network and its layers.\n",
    "\n",
    "        Args:\n",
    "            seed (int): Random seed for reproducibility. Default is 42.\n",
    "\n",
    "        Notes:\n",
    "            The network's weights and biases are initialized in their own init functions using He initialization.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.L1 = LinearLayer(784, 26)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        self.layers = [self.L1, self.softmax]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the entire network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (784, m), where m is the number of examples.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output probabilities from the network with shape (n_classes, m).\n",
    "                           Here n_classes is 26.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the output of the network\n",
    "\n",
    "        return X\n",
    "\n",
    "    def cross_entropy(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix of shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels of shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: The average cross-entropy loss over all m examples.\n",
    "\n",
    "        Notes:\n",
    "            A small constant epsilon is added to Y_hat to avoid computing log(0).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the cross-entropy loss\n",
    "        pass\n",
    "\n",
    "    def convert_prob_into_class(self, probs):\n",
    "        \"\"\"\n",
    "        Convert predicted probability distributions into class labels.\n",
    "\n",
    "        Args:\n",
    "            probs (numpy.ndarray): Predicted probabilities with shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Array of predicted class labels with shape (m,).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Convert the probabilities into a class\n",
    "        pass\n",
    "\n",
    "    def get_accuracy(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Compute the classification accuracy.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted probability matrix from the network, shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy as a fraction between 0 and 1.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the accuracy of the network\n",
    "        pass\n",
    "\n",
    "    def backprop(self, Y_hat, Y):\n",
    "        \"\"\"\n",
    "        Perform backpropagation over the entire network to compute gradients.\n",
    "\n",
    "        Args:\n",
    "            Y_hat (numpy.ndarray): Predicted output probabilities, shape (n_classes, m).\n",
    "            Y (numpy.ndarray): One-hot encoded true labels, shape (n_classes, m).\n",
    "\n",
    "        Process:\n",
    "            Starts by computing the derivative of the cross-entropy loss with respect to the final layer\n",
    "            and then propagate the gradients backward through all layers.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Calculate the gradient of the loss with respect to the input\n",
    "        pass\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate, batch_size=32, verbose=False):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data with shape (784, m), where each column is a flattened MNIST style image.\n",
    "            Y (numpy.ndarray): One-hot encoded labels with shape (n_classes, m), where n_classes is 26\n",
    "            epochs (int): Number of epochs for training.\n",
    "            learning_rate (float): Learning rate for the parameter updates.\n",
    "            batch_size (int, optional): Number of examples per mini-batch. Default is 32.\n",
    "            verbose (bool, optional): If True, prints training progress every 500 epochs. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing:\n",
    "                - 'loss_history': List of loss values for each epoch.\n",
    "                - 'accuracy_history': List of accuracy values for each epoch.\n",
    "\n",
    "        Process:\n",
    "            - Shuffles the dataset each epoch.\n",
    "            - Processes data in mini-batches.\n",
    "            - Performs a forward pass, backpropagation, and parameter updates for each mini-batch.\n",
    "            - Computes the loss and accuracy for the entire dataset after each epoch.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        accuracy_history = []\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            # Mini-batch processing\n",
    "            permutation = np.random.permutation(m)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[:, permutation]\n",
    "            \n",
    "            for j in range(0, m, batch_size):\n",
    "                X_batch = X_shuffled[:, j:j+batch_size]\n",
    "                Y_batch = Y_shuffled[:, j:j+batch_size]\n",
    "                \n",
    "                # Forward propagation\n",
    "                # TODO: Calculate the output of the network\n",
    "                \n",
    "                # Backward propagation\n",
    "                # TODO: Calculate the gradients of the loss with respect to the input\n",
    "                \n",
    "                # Update parameters\n",
    "                # TODO: Update the weights and biases of the layer using the learning rate\n",
    "            \n",
    "            # Calculate metrics for the whole epoch\n",
    "            Y_hat_full = self.forward(X)\n",
    "            loss = self.cross_entropy(Y_hat_full, Y)\n",
    "            accuracy = self.get_accuracy(Y_hat_full, Y)\n",
    "            \n",
    "            loss_history.append(loss)\n",
    "            accuracy_history.append(accuracy)\n",
    "            \n",
    "            if verbose and i % 500 == 0:\n",
    "                print(f\"Epoch {i+1}/{epochs}\")\n",
    "                print(f\"loss: {loss:.5f}\")\n",
    "                print(f\"accuracy: {accuracy:.5f}\")\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        return {'loss_history': loss_history, 'accuracy_history': accuracy_history}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
