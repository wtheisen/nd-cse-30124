{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_HwBNlcqzrs"
      },
      "source": [
        "### Homework 04: Optical Character Recognition\n",
        "\n",
        "Now that you have the segmented letters from the previous task, we need a way to actually convert the letters to text! You can't be bothered to just transcribe the images yourself, but you remember your professor droning on about something called MNIST and you think that these letters might be kind of similar to handwritten digits.\n",
        "\n",
        "Unfortunately, because your professor hates you, he's making you write a FFN using only numpy for the first part of this assignment. Use the dataset available from the following link for training, testing, and validation on this assignment. [Alphabet Cuttings Dataset](https://drive.google.com/drive/folders/1xK3Mp9BhXWpae-ZicfGtTqkVRW-x8ntI?usp=sharing)\n",
        "\n",
        "The code immediately below is for loading and formatting the dataset. You don't have to do anything here yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-HeldcPqzrt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def detect_rgb_contours(input_path, display=False):\n",
        "    \"\"\"\n",
        "    Detect contours in the RGB channels of a PNG image and draw all contours in hierarchy.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the input PNG image\n",
        "        line_thickness (int): Thickness of contour lines in pixels\n",
        "    \"\"\"\n",
        "    # Read the image with alpha channel\n",
        "    img = cv2.imread(input_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    # Extract the RGB channels\n",
        "    rgb_img = img[:, :, :3]\n",
        "\n",
        "    # Convert to grayscale for contour detection\n",
        "    gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n",
        "    if display:\n",
        "        display(Image.fromarray(gray))\n",
        "\n",
        "    # Setting parameter values\n",
        "    t_lower = 50  # Lower Threshold\n",
        "    t_upper = 150  # Upper threshold\n",
        "\n",
        "    # Applying the Canny Edge filter\n",
        "    edge = cv2.Canny(gray, t_lower, t_upper)\n",
        "    # Close the edges to form complete contours\n",
        "    if display:\n",
        "        display(Image.fromarray(edge))\n",
        "\n",
        "    # Find contours recursively\n",
        "    contours, hierarchy = cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Create a separate image for each contour with different colors\n",
        "    mnist_img = np.zeros((rgb_img.shape[0], rgb_img.shape[1]), dtype=np.uint8)\n",
        "    if display:\n",
        "        print(mnist_img.shape)\n",
        "        print(len(hierarchy))\n",
        "\n",
        "        print(hierarchy)\n",
        "\n",
        "    # Generate a different color for each contour based on index\n",
        "    for i, contour in enumerate(contours):\n",
        "        if i == 1:\n",
        "            cv2.drawContours(mnist_img, [contour], -1, 0, thickness=cv2.FILLED)\n",
        "        elif i % 2 == 1:\n",
        "            cv2.drawContours(mnist_img, [contour], -1, 255, thickness=cv2.FILLED)\n",
        "\n",
        "    mnist_img = cv2.resize(mnist_img, (28, 28))\n",
        "    # Display the result with multiple contours\n",
        "    if display:\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(mnist_img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"All {len(contours)} contours with unique colors\")\n",
        "        plt.show()\n",
        "\n",
        "    return mnist_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnf5vqf8qzrt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from typing import Dict, Tuple\n",
        "from IPython.display import display\n",
        "\n",
        "def load_letter_dataset(data_dir: str, train_size: int = 7, test_size: int = 2, holdout_size: int = 1) -> Dict:\n",
        "    \"\"\"\n",
        "    Load and split letter dataset into train, test, and holdout sets.\n",
        "    \"\"\"\n",
        "    # Verify split sizes\n",
        "    assert train_size + test_size + holdout_size == 10, \"Split sizes must sum to 10\"\n",
        "\n",
        "    # Dictionary to store all instances of each letter\n",
        "    letter_instances = defaultdict(list)\n",
        "\n",
        "    # Collect all image paths\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith('.png') and not filename[0].isdigit():\n",
        "            letter = filename[0]  # First character is the letter\n",
        "            instance_path = os.path.join(data_dir, filename)\n",
        "            letter_instances[letter].append(instance_path)\n",
        "\n",
        "    train_data = {'images': [], 'labels': []}\n",
        "    test_data = {'images': [], 'labels': []}\n",
        "    holdout_data = {'images': [], 'labels': []}\n",
        "\n",
        "    # Process each letter\n",
        "    for letter, instances in letter_instances.items():\n",
        "        # Randomly shuffle the instances\n",
        "        random.shuffle(instances)\n",
        "\n",
        "        # Split into train/test/holdout\n",
        "        train_paths = instances[:train_size]\n",
        "        test_paths = instances[train_size:train_size + test_size]\n",
        "        holdout_paths = instances[train_size + test_size:]\n",
        "\n",
        "        # Load images and add to respective sets\n",
        "        for path in train_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            train_data['images'].append(img)\n",
        "            train_data['labels'].append(letter)\n",
        "\n",
        "        for path in test_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            test_data['images'].append(img)\n",
        "            test_data['labels'].append(letter)\n",
        "\n",
        "        for path in holdout_paths:\n",
        "            img = detect_rgb_contours(path)\n",
        "            holdout_data['images'].append(img)\n",
        "            holdout_data['labels'].append(letter)\n",
        "\n",
        "    print(train_data['labels'][0], train_data['images'][0].shape)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(train_data['images'][0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    for dataset in [train_data, test_data, holdout_data]:\n",
        "        dataset['images'] = np.array(dataset['images'])\n",
        "        dataset['labels'] = np.array(dataset['labels'])\n",
        "\n",
        "    return {\n",
        "        'train': train_data,\n",
        "        'test': test_data,\n",
        "        'holdout': holdout_data\n",
        "    }\n",
        "\n",
        "def prepare_data(data_dict: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Prepare data for FFN training:\n",
        "    - Preprocess all images\n",
        "    - Convert labels to numerical format\n",
        "    - Split into features (X) and labels (y)\n",
        "    \"\"\"\n",
        "    # Process training data\n",
        "    X_train = np.array([img.reshape(-1) / 255 for img in data_dict['train']['images']])\n",
        "    X_test = np.array([img.reshape(-1) / 255 for img in data_dict['test']['images']])\n",
        "\n",
        "    # Convert labels to numerical format\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(data_dict['train']['labels'])\n",
        "    y_test = label_encoder.transform(data_dict['test']['labels'])\n",
        "\n",
        "    # Save label encoder mapping for reference\n",
        "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "    print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeyqaOozqzru"
      },
      "source": [
        "**Neural Network from Scratch**\n",
        "\n",
        "Your task is to implement a simple neural network from scratch in numpy to classify the letters in the dataset following the architecture shown below.\n",
        "\n",
        "In order to actually implement a training regime for our network, we'll need to specify a loss function that we can use to measure how well our network is doing. We'll use the cross entropy loss function as we're attempting a multiclass classification task.\n",
        "\n",
        "![Cross Entropy Loss](https://pbs.twimg.com/media/FBmVmdHWQAAU7gq.png)\n",
        "\n",
        "Training our network will consist of two steps primarily, forward propagation and back propagation.\n",
        "\n",
        "Forward propagation is the process of taking our input data, and passing it through the network to get a prediction.\n",
        "\n",
        "Back propagation is the process of taking the derivative of the loss function with respect to the weights and biases, and using gradient descent to update the weights and biases.\n",
        "\n",
        "![NN Training](https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/blueprint.gif)\n",
        "\n",
        "In this gif we can see a brief outline of the forward and backward propagation steps.\n",
        "\n",
        "Broadly speaking, forward is what gives us our prediction, and backward is what gives us the gradient of the loss function with respect to the weights and biases, and is how we update the weights to get closer to the right answer (done by minimizing the loss function)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CuYXmZZqzru"
      },
      "source": [
        "We'll also need to implement a couple activation functions and their derivatives.\n",
        "\n",
        "We're going to be using the ReLU activation function for our hidden layers, and a softmax function for our output layer. The softmax will allow us to map our output to a probability between 0 and 1 and from there to a class based on an argmax operation.\n",
        "\n",
        "![Activation Functions](https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/activations.gif)\n",
        "\n",
        "Here we can see both activation functions and their derivatives.\n",
        "\n",
        "The part that most people find tricky about this is the backpropagation step.\n",
        "\n",
        "As we've seen in class for \"single layer\" examples, to optimize the weights of a model using gradient descent, we can rewrite the loss function in terms of the weights and then take partial derivatives with respect to each weight.\n",
        "\n",
        "![Gradient Descent](https://global.discourse-cdn.com/dlai/original/3X/f/5/f58df86a4c92695569d9536d7e752161cd0f98fb.jpeg)\n",
        "\n",
        "Will multilayer networks, how do we take the derivative of the loss function with respect to the weights, if the weights in the previous layer are reliant on the weights in the layer before them?\n",
        "\n",
        "Backpropagation is the solution to this and revolves around using the chain rule to take essentially a series of partial derivatives backwards through the network to get the gradient of the loss function with respect to the weights at each layer. We can then redistribute these gradients to update the weights of the network.\n",
        "\n",
        "![Backprop](https://miro.medium.com/v2/resize:fit:1200/0*9lo2ux8ASvt6YJkH.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CX7m-iAqzru"
      },
      "source": [
        "**BETTER TEACHING**\n",
        "\n",
        "To be honest, your best bet is to watch the youtube videos by 3Blue1Brown. He's an incredible teacher and will do a better job than I can, along with better visualizations.\n",
        "\n",
        "[![Introduction to Neural Networks](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n",
        "\n",
        "This is an introduction to neural networks using the MNIST dataset!\n",
        "\n",
        "Then we have a great video on gradient descent.\n",
        "\n",
        "[![Gradient Descent](https://img.youtube.com/vi/IHZwWFHWa-w/0.jpg)](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
        "\n",
        "Finally I'd recommend at least his first video on backpropagation, though you should probably watch the second too.\n",
        "\n",
        "[![Backprop](https://img.youtube.com/vi/Ilg3gGewQ5U/0.jpg)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_sCyMDgqzru"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NumpyNeuralNetwork:\n",
        "    # Here we define the number and types of layers in our network\n",
        "    # we also include their activation functions\n",
        "    # TODO: You'll almost certainly need to add some more layers to get to 70% accuracy\n",
        "    NN_ARCHITECTURE = [\n",
        "        {\"input_dim\": 784, \"output_dim\": 37, \"activation\": \"relu\"},\n",
        "        {\"input_dim\": 37, \"output_dim\": 26, \"activation\": \"softmax\"},\n",
        "    ]\n",
        "\n",
        "    # Our init function just initializes the weights and biases for each layer\n",
        "    def __init__(self, seed = 42):\n",
        "        # random seed initiation\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # parameters storage initiation\n",
        "        self.params_values = {}\n",
        "\n",
        "        # iteration over network layers\n",
        "        for idx, layer in enumerate(self.NN_ARCHITECTURE):\n",
        "            # we number network layers from 1\n",
        "            layer_idx = idx + 1\n",
        "\n",
        "            # extracting the number of units in layers\n",
        "            layer_input_size = layer[\"input_dim\"]\n",
        "            layer_output_size = layer[\"output_dim\"]\n",
        "\n",
        "            # initiating the values of the W matrix\n",
        "            # and vector b for subsequent layers\n",
        "            self.params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "                layer_output_size, layer_input_size) * 0.1\n",
        "            self.params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "                layer_output_size, 1) * 0.1\n",
        "\n",
        "    def get_params_values(self):\n",
        "        return self.params_values\n",
        "\n",
        "    # TODO: Write the relu function\n",
        "    def relu(self, Z):\n",
        "      \"\"\"\n",
        "        Applies the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "        Inputs:\n",
        "           - Z: NumPy array of pre-activation values from a layer\n",
        "\n",
        "        Returns:\n",
        "           - A: NumPy array with ReLU outputs\n",
        "\n",
        "        Concept Check: Why is Z in a perceptron a number but Z in a neural network is a matrix of numbers?\n",
        "\n",
        "        Concept Check: What are the dimensions of the Z vector? Don't answer with a specific number but a generalizable statement\n",
        "      \"\"\"\n",
        "        return None\n",
        "\n",
        "    # TODO: Write the relu_backward function\n",
        "    def relu_backward(self, dA, Z):\n",
        "      \"\"\"\n",
        "        Perform the backward pass for the ReLU activation function.\n",
        "\n",
        "        Inputs:\n",
        "          - dA: Gradient of the loss with respect to the activation output (A) from the current layer\n",
        "          - Z:  The input to the activation function of currently layer\n",
        "\n",
        "        Returns:\n",
        "           - dZ: Gradient of the loss with respect to the input (Z) of the current layers ReLU activation function\n",
        "\n",
        "        Concept Check: What is the purpose of setting the gradient dZ to 0 for elements where Z≤0 in the ReLU backward function?\n",
        "\n",
        "        Concept Check: What is the calculated dZ (the returned matrix) of this function used for?\n",
        "      \"\"\"\n",
        "        return None\n",
        "\n",
        "    # TODO: Write the softmax function\n",
        "    def softmax(self, Z):\n",
        "      \"\"\"\n",
        "        Computes the softmax activation function for the given input Z.\n",
        "\n",
        "        Inputs:\n",
        "          - Z : Input matrix to the softmax function\n",
        "\n",
        "        Returns:\n",
        "          - A probability distribution representing the likelihood of each class\n",
        "\n",
        "        Concept Check: What does the softmax function do and where is it normally used in a neural network?\n",
        "      \"\"\"\n",
        "        return None\n",
        "\n",
        "    # TODO: Write the softmax_backward function\n",
        "    def softmax_backward(self, dA, Z):\n",
        "      \"\"\"\n",
        "        Computes the gradient of the loss with respect to Z for a softmax activation function.\n",
        "\n",
        "        Inputs:\n",
        "          - dA: Gradient of the loss with respect to the output of the softmax layer\n",
        "          - Z: Input to the softmax function before activation\n",
        "\n",
        "        Returns:\n",
        "          - Gradient of the loss with respect to Z\n",
        "      \"\"\"\n",
        "      # Hint: for cross entropy loss function, softmax_backwards becames very simple (1 line)\n",
        "        return None\n",
        "\n",
        "    # TODO: Finish the single_layer_forward_propagation function\n",
        "    def single_layer_forward_propagation(self, A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "        \"\"\"\n",
        "        Performs forward propagation for a single layer.\n",
        "\n",
        "        Parameters:\n",
        "          - A_prev: Activation from the previous layer\n",
        "          - W_curr: Weights for the current layer\n",
        "          - b_curr: Biases for the current layer\n",
        "          - activation: Activation function to apply\n",
        "\n",
        "        Returns:\n",
        "          - A: Activation output of the current layer\n",
        "          - Z_curr: linear transformation result before activation\n",
        "\n",
        "        Concept Check: Why do we return both A and Z_curr?\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: calculation of the input value for the activation function\n",
        "        #    hint: this looks super similar to the perceptron equation!\n",
        "        Z_curr = None\n",
        "\n",
        "        # selection of activation function\n",
        "        if activation == \"relu\":\n",
        "            activation_func = self.relu\n",
        "        elif activation == \"sigmoid\":\n",
        "            activation_func = self.sigmoid\n",
        "        else:\n",
        "            raise Exception('Non-supported activation function')\n",
        "\n",
        "        # TODO: return of calculated activation A and the intermediate Z matrix\n",
        "        return None\n",
        "\n",
        "    # TODO: Finish the full_forward_propagation function\n",
        "    def full_forward_propagation(self, X):\n",
        "      \"\"\"\n",
        "        Performs forward propagation through the entire neural network.\n",
        "\n",
        "        Inputs:\n",
        "           - X : input data\n",
        "\n",
        "        Returns:\n",
        "          - A_curr : final activation output of the network\n",
        "          - memory : dictionary storing intermediate A and Z values for backpropagation\n",
        "      \"\"\"\n",
        "        # creating a temporary memory to store the information needed for a backward step\n",
        "        memory = {}\n",
        "        # X vector is the activation for layer 0\n",
        "        A_curr = X\n",
        "\n",
        "        # iteration over network layers\n",
        "        for idx, layer in enumerate(self.NN_ARCHITECTURE):\n",
        "            # we number network layers from 1\n",
        "            layer_idx = idx + 1\n",
        "            # transfer the activation from the previous iteration\n",
        "            A_prev = A_curr\n",
        "\n",
        "            # TODO: extraction of the activation function for the current layer\n",
        "            activ_function_curr = None\n",
        "            # TODO: extraction of W for the current layer\n",
        "            W_curr = None\n",
        "            # TODO: extraction of b for the current layer\n",
        "            b_curr = None\n",
        "            # TODO: calculation of activation for the current layer\n",
        "            A_curr, Z_curr = None\n",
        "\n",
        "            # saving calculated values in the memory\n",
        "            memory[\"A\" + str(idx)] = A_prev\n",
        "            memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "\n",
        "        # return of prediction vector and a dictionary containing intermediate values\n",
        "        return A_curr, memory\n",
        "\n",
        "    def get_cost_value(self, Y_hat, Y):\n",
        "      \"\"\"\n",
        "        Computes the cost of the neural network's predictions\n",
        "\n",
        "        Inputs:\n",
        "          - Y_hat: The predicted probabilities\n",
        "          - Y:     ground truth labels\n",
        "\n",
        "        Output:\n",
        "          - The computed loss\n",
        "\n",
        "        Concept Check: What are the dimensions of Y_hat and Y?\n",
        "      \"\"\"\n",
        "\n",
        "        # number of examples\n",
        "        m = Y_hat.shape[1]\n",
        "\n",
        "        # calculation of the cost according to the formula\n",
        "        cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    # TODO: Write the convert_prob_into_class function\n",
        "    def convert_prob_into_class(self, probs):\n",
        "      \"\"\"\n",
        "        Converts probability values from the softmax function into discrete class predictions\n",
        "\n",
        "        Inputs:\n",
        "          - probs : 2D array where each col represents the predicted probability distribution\n",
        "                    over classes for a given example\n",
        "        Output:\n",
        "          - probs_ : 1D array where each value represents the predicted class for each example\n",
        "      \"\"\"\n",
        "        probs_ = np.copy(probs)\n",
        "        pass\n",
        "\n",
        "        return probs_.flatten()\n",
        "\n",
        "    def get_accuracy_value(self, Y_hat, Y):\n",
        "      \"\"\"\n",
        "        Computes the accuracy of the model's predictions by comparing them with the true labels\n",
        "\n",
        "        Inputs:\n",
        "          - Y_hat:\n",
        "          - Y:\n",
        "\n",
        "        Output:\n",
        "          - The accuracy of the model’s predictions, which is the fraction of correctly predicted labels\n",
        "      \"\"\"\n",
        "        Y_hat_ = self.convert_prob_into_class(Y_hat)\n",
        "        return (Y_hat_ == Y).all(axis=0).mean()\n",
        "\n",
        "    # TODO: Write the single_layer_backward_propagation function\n",
        "    def single_layer_backward_propagation(self, dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "      \"\"\"\n",
        "        Performs backward propagation for a single layer to calculate the gradients of the cost function\n",
        "        with respect to the weights, biases, and activations\n",
        "\n",
        "        Inputs:\n",
        "           - dA_curr: The gradient of the loss with respect to the activation output (A) from the current layer\n",
        "           - W_curr : The weights for the current layer\n",
        "           - b_curr : The biases for the current layer\n",
        "           - Z_curr : The linear transformation result (Z) before activation for the current layer\n",
        "           - A_prev : The activation from the previous layer\n",
        "           - activation : The activation function used in the current layer (\"relu\" or \"sigmoid\")\n",
        "\n",
        "        Output:\n",
        "           - dA_prev : The gradient of the loss with respect to the activation of the previous layer (used for backpropagation)\n",
        "           - dW_curr : The gradient of the cost function with respect to the weights (used for weight updates)\n",
        "           - db_curr : The gradient of the cost function with respect to the biases  (used for bias updates)\n",
        "\n",
        "        Concept Check: What is the significance of calculating dW_curr in backpropagation?\n",
        "\n",
        "\n",
        "        Concept Check: After calculating the gradients for weights (dW_curr) and biases (db_curr), what is the purpose\n",
        "                       of the calculation dA_prev used for in backpropagation?\n",
        "      \"\"\"\n",
        "\n",
        "        # number of examples\n",
        "        m = A_prev.shape[1]\n",
        "\n",
        "        # selection of activation function\n",
        "        if activation == \"relu\":\n",
        "            backward_activation_func = self.relu_backward\n",
        "        elif activation == \"sigmoid\":\n",
        "            backward_activation_func = self.sigmoid_backward\n",
        "        else:\n",
        "            raise Exception('Non-supported activation function')\n",
        "\n",
        "        # TODO: calculation of the activation function derivative\n",
        "        dZ_curr = None\n",
        "        # TODO: derivative of the matrix W\n",
        "        dW_curr = None\n",
        "        # TODO: derivative of the vector b\n",
        "        db_curr = None\n",
        "        # TODO: derivative of the matrix A_prev\n",
        "        dA_prev = None\n",
        "\n",
        "        return dA_prev, dW_curr, db_curr\n",
        "\n",
        "    # TODO: Finish the full_backward_propagation function\n",
        "    def full_backward_propagation(self, Y_hat, Y, memory):\n",
        "      \"\"\"\n",
        "        Performs the backward propagation through the entire neural network.\n",
        "\n",
        "        Inputs:\n",
        "          - Y_hat: the predicted values (activations)\n",
        "          - Y:     ground truth labels (one-hot encoded)\n",
        "          - memory: dictionary containing the activations and pre-activations (Z values)\n",
        "                for each layer during the forward pass.\n",
        "\n",
        "        Outputs:\n",
        "          - grads_values: dictionary containing the gradients of the cost function\n",
        "                          with respect to the weights, biases, and activations for each layer.\n",
        "\n",
        "        Concept Check: Why store the calculated gradients in a dictionary? How will they be used?\n",
        "      \"\"\"\n",
        "        grads_values = {}\n",
        "\n",
        "        # number of examples\n",
        "        m = Y.shape[1]\n",
        "        # a hack ensuring the same shape of the prediction vector and labels vector\n",
        "        Y = Y.reshape(Y_hat.shape)\n",
        "\n",
        "        # TODO: initiation of gradient descent algorithm\n",
        "           # hint: The initial gradient of the loss with respect to the activation can be set up using only the the predicted labels, true lables, and one mathmatical operator\n",
        "        dA_prev = None\n",
        "\n",
        "        # iteration over network layers\n",
        "        for layer_idx_prev, layer in reversed(list(enumerate(self.NN_ARCHITECTURE))):\n",
        "            # we number network layers from 1\n",
        "            layer_idx_curr = layer_idx_prev + 1\n",
        "\n",
        "            # extraction of the activation function for the current layer\n",
        "            activ_function_curr = layer[\"activation\"]\n",
        "\n",
        "            dA_curr = dA_prev\n",
        "\n",
        "            # We get the activation from the previous layer and the Z matrix from the current layer\n",
        "            A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "            Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "\n",
        "            # We get the weights and biases for the current layer\n",
        "            W_curr = self.params_values[\"W\" + str(layer_idx_curr)]\n",
        "            b_curr = self.params_values[\"b\" + str(layer_idx_curr)]\n",
        "\n",
        "            # TODO: calculate the gradients of the cost function with respect to the weights and biases\n",
        "            dA_prev, dW_curr, db_curr = None\n",
        "\n",
        "            # We save the gradients of the cost function with respect to the weights and biases\n",
        "            grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "            grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "\n",
        "        return grads_values\n",
        "\n",
        "    def update(self, grads_values):\n",
        "      \"\"\"\n",
        "        Updates the weights and biases of the neural network during gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "          - grads_values: dictionary containing the previously calculated gradients\n",
        "\n",
        "        Outputs:\n",
        "          - params_values: dictionary containing the updated values of the weights and biases\n",
        "      \"\"\"\n",
        "        # iteration over network layers\n",
        "        for layer_idx, layer in enumerate(self.NN_ARCHITECTURE, 1):\n",
        "            self.params_values[\"W\" + str(layer_idx)] -= self.learning_rate * grads_values[\"dW\" + str(layer_idx)]\n",
        "            self.params_values[\"b\" + str(layer_idx)] -= self.learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "        return self.params_values\n",
        "\n",
        "    # TODO: Finish the train function\n",
        "    def train(self, X, Y, epochs=100, learning_rate=0.01, batch_size=8, verbose=False):\n",
        "      \"\"\"\n",
        "        Train the neural network using mini-batch gradient descent\n",
        "\n",
        "        Inputs:\n",
        "          - X: Input data (features), shape (n_features, n_examples)\n",
        "          - Y: True labels, shape (n_classes, n_examples)\n",
        "          - epochs: Number of training iterations\n",
        "          - learning_rate: Learning rate for gradient descent\n",
        "          - batch_size: Size of each mini-batch\n",
        "          - verbose: If True, prints cost and accuracy at intervals\n",
        "\n",
        "        Outputs:\n",
        "          - Dictionary containing cost and accuracy history over epochs\n",
        "    \"\"\"\n",
        "        # initiation of lists storing the history of metrics calculated during the learning process\n",
        "        cost_history = []\n",
        "        accuracy_history = []\n",
        "        m = X.shape[1]\n",
        "\n",
        "        # TODO: implement mini-batch training\n",
        "        for i in range(epochs):\n",
        "            # Mini-batch processing\n",
        "            permutation = np.random.permutation(m)\n",
        "            X_shuffled = X[:, permutation]\n",
        "            Y_shuffled = Y[:, permutation]\n",
        "\n",
        "            for j in range(0, m, batch_size):\n",
        "\n",
        "                #TODO: Forward propagation\n",
        "                pass\n",
        "\n",
        "                #TODO: Backward propagation\n",
        "                pass\n",
        "\n",
        "                #TODO: Update parameters\n",
        "                self.update(grads, learning_rate)\n",
        "\n",
        "            # TODO: Calculate metrics for the whole epoch (cost and accuracy)\n",
        "\n",
        "\n",
        "            # Append metrics to storage\n",
        "            cost_history.append(cost)\n",
        "            accuracy_history.append(accuracy)\n",
        "\n",
        "            if verbose and i % 500 == 0:\n",
        "                print(f\"Epoch {i+1}/{epochs}\")\n",
        "                print(f\"Cost: {cost:.5f}\")\n",
        "                print(f\"Accuracy: {accuracy:.5f}\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "        return {'cost_history': cost_history, 'accuracy_history': accuracy_history}\n",
        "\n",
        "\n",
        "# Comment to prevent docstrings from being printed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRdFPk8aqzrv"
      },
      "source": [
        "### FFN Evaluation\n",
        "\n",
        "The cell below will allow you to evaluate the performance of your FFN on the holdout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Z7gBmCqzrw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_on_holdout(data_dict, model):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the holdout set\n",
        "\n",
        "    Args:\n",
        "        data_dict: Dictionary containing the dataset splits\n",
        "        model: Trained NumpyNeuralNetwork model\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy on holdout set\n",
        "        np.ndarray: Confusion matrix\n",
        "    \"\"\"\n",
        "    # Preprocess holdout data\n",
        "    X_holdout = np.array([img.reshape(-1) / 255 for img in data_dict['holdout']['images']])\n",
        "\n",
        "    # Get labels and convert to numerical format using the same encoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(data_dict['train']['labels'])  # Fit on training data to maintain same mapping\n",
        "    y_holdout = label_encoder.transform(data_dict['holdout']['labels'])\n",
        "\n",
        "    # Convert to format needed by model\n",
        "    X_holdout = X_holdout.T\n",
        "    y_holdout_onehot = np.eye(26)[y_holdout].T\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred, _ = model.full_forward_propagation(X_holdout)\n",
        "    accuracy = model.get_accuracy_value(y_pred, y_holdout_onehot)\n",
        "\n",
        "    # Get predicted classes\n",
        "    predicted_classes = np.argmax(y_pred, axis=0)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    conf_matrix = confusion_matrix(y_holdout, predicted_classes)\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nHoldout Set Evaluation:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return accuracy, conf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J0tN8V2qzrw"
      },
      "source": [
        "### Running our FNN\n",
        "\n",
        "Lets use all of our data to train and evaluate our FFN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0NX_Y6qzrw"
      },
      "outputs": [],
      "source": [
        "data = load_letter_dataset(\"homework04/alphabet\")\n",
        "X_train, X_test, y_train, y_test = prepare_data(data)\n",
        "\n",
        "# Convert to proper format\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "y_train_onehot = np.eye(26)[y_train].T\n",
        "y_test_onehot = np.eye(26)[y_test].T\n",
        "\n",
        "# Initialize and train model\n",
        "model = NumpyNeuralNetwork()\n",
        "history = model.train(X_train, y_train_onehot, batch_size=32, verbose=True)\n",
        "\n",
        "# Evaluate on holdout set\n",
        "holdout_accuracy, conf_matrix = evaluate_on_holdout(data, model)\n",
        "\n",
        "# Visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['cost_history'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['accuracy_history'])\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix on Holdout Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEs1r9Elqzrw"
      },
      "source": [
        "### Target Accuracy: 70% on Holdout Set\n",
        "\n",
        "Instead of giving hard values, which is basically impossible in deep learning, I'll be giving you a target output accuracy instead. Your goal is to reach 70% accuracy on the holdout set. You'll almost certainly have to test a number of different combinations of architectures and hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4bMTRN5qzrw"
      },
      "source": [
        "### CNN Experiment\n",
        "\n",
        "While the FFN is okay, it's really not that well suited to image classification tasks such as this. Fighting through the hangover, you recall something about the news channel CNN? Implement a CNN (using pytorch) below and see if you can get a better result than the FFN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHOavdLHqzrw"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports for both experiments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class LetterDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "\n",
        "        # Use LabelEncoder to encode the labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.labels = self.label_encoder.fit_transform(labels)  # Fit and transform labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image and label\n",
        "        image = Image.fromarray(self.images[idx], mode='L')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transform to image if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "#TODO: Define the neural network architecture\n",
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, num_classes=26):  # Assuming 26 classes (A-Z)):\n",
        "      \"\"\"\n",
        "        Basic CNN for letter classification.\n",
        "\n",
        "        Inputs:\n",
        "          - num_classes: Number of output classes (default: 26 for A-Z).\n",
        "\n",
        "        Output:\n",
        "          - Logits (before softmax) representing class predictions.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "        Forward pass of the CNN.\n",
        "\n",
        "        Inputs:\n",
        "          - x: Input image tensor of shape (batch_size, 1, 28, 28).\n",
        "\n",
        "        Output:\n",
        "          - Logits for classification.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "#TODO: Training function for the CNN\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=100):\n",
        "  \"\"\"\n",
        "    Trains a CNN model using mini-batch gradient descent and evaluates it on a validation set.\n",
        "\n",
        "    Inputs:\n",
        "      - model: The neural network model to be trained\n",
        "      - train_loader: DataLoader for the training dataset\n",
        "      - val_loader: DataLoader for the validation dataset\n",
        "      - device: The device (CPU or GPU) to run training on\n",
        "      - num_epochs: Number of epochs for training\n",
        "      - learning_rate: learning rate\n",
        "\n",
        "    Outputs:\n",
        "      - Dictionary containing training loss, training accuracy, and validation accuracy history\n",
        "  \"\"\"\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Train the model\n",
        "            pass\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "\n",
        "        # TODO: Validate model on validation set\n",
        "        pass\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '\n",
        "                f'Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQkxyr9zqzrw"
      },
      "outputs": [],
      "source": [
        "# Load data (using your existing load_letter_dataset function)\n",
        "data_dict = load_letter_dataset(\"homework_datasets/alphabet\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LetterDataset(data_dict['train']['images'],\n",
        "                                data_dict['train']['labels'],\n",
        "                                transform=transform)\n",
        "val_dataset = LetterDataset(data_dict['test']['images'],\n",
        "                            data_dict['test']['labels'],\n",
        "                            transform=transform)\n",
        "holdout_dataset = LetterDataset(data_dict['holdout']['images'],\n",
        "                                data_dict['holdout']['labels'],\n",
        "                                transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=32)\n",
        "\n",
        "# Initialize model and training components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BasicCNN().to(device)\n",
        "\n",
        "# Train model\n",
        "train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "# Evaluate on holdout set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in holdout_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "holdout_acc = 100. * correct / total\n",
        "print(f'Holdout Accuracy: {holdout_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecHzhz5Mqzrx"
      },
      "source": [
        "### Target Accuracy: 80% on Holdout Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH0iEqP6qzrx"
      },
      "source": [
        "Unfortunately, despite having the text, you still can't read it. It appears to be encoded with some kind of cipher. If only there were seq2seq models that you maybe could use to decode it..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}