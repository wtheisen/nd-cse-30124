{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (56000,10) and (64,128) not aligned: 10 (dim 1) != 64 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m z1, a1, z2, a2, z3, a3, z4, a4, z5, a5 \u001b[38;5;241m=\u001b[39m forward(x_train)\n\u001b[1;32m    117\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(a5, y_train)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 86\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(X, y, z1, a1, z2, a2, z3, a3, z4, a4, z5, a5)\u001b[0m\n\u001b[1;32m     83\u001b[0m db5 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(error_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Hidden layer error\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m error_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigmoid_derivative(a4)\n\u001b[1;32m     87\u001b[0m dW4 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a3\u001b[38;5;241m.\u001b[39mT, error_hidden)\n\u001b[1;32m     88\u001b[0m db4 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(error_hidden, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (56000,10) and (64,128) not aligned: 10 (dim 1) != 64 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST data from sklearn's openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to integers and one-hot encode them\n",
    "y_train = y_train.astype(int).to_numpy()  # Convert to NumPy array\n",
    "y_test = y_test.astype(int).to_numpy()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Neural network parameters\n",
    "input_size = 784\n",
    "h1_size = 512\n",
    "h2_size = 256\n",
    "h3_size = 128\n",
    "h4_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Initialize weights and biases with values and gradients (dual numbers)\n",
    "W1 = np.random.randn(input_size, h1_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros((1, h1_size))\n",
    "W2 = np.random.randn(h1_size, h2_size) * np.sqrt(2. / h1_size)\n",
    "b2 = np.zeros((1, h2_size))\n",
    "W3 = np.random.randn(h2_size, h3_size) * np.sqrt(2. / h2_size)\n",
    "b3 = np.zeros((1, h3_size))\n",
    "W4 = np.random.randn(h3_size, h4_size) * np.sqrt(2. / h3_size)\n",
    "b4 = np.zeros((1, h4_size))\n",
    "W5 = np.random.randn(h4_size, output_size) * np.sqrt(2. / h4_size)\n",
    "b5 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "    z5 = np.dot(a4, W5) + b5\n",
    "    a5 = softmax(z5)\n",
    "    return z1, a1, z2, a2, z3, a3, z4, a4, z5, a5\n",
    "\n",
    "# Backward pass\n",
    "def backward(X, y, z1, a1, z2, a2, z3, a3, z4, a4, z5, a5):\n",
    "    global W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\n",
    "\n",
    "    # Output layer error\n",
    "    error_output = a5 - y\n",
    "    dW5 = np.dot(a4.T, error_output)\n",
    "    db5 = np.sum(error_output, axis=0, keepdims=True)\n",
    "\n",
    "    # Hidden layer error\n",
    "    error_hidden = np.dot(error_output, W4.T) * sigmoid_derivative(a4)\n",
    "    dW4 = np.dot(a3.T, error_hidden)\n",
    "    db4 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    error_hidden = np.dot(error_output, W3.T) * sigmoid_derivative(a3)\n",
    "    dW3 = np.dot(a2.T, error_hidden)\n",
    "    db3 = np.sum(error_hidden, axis=0, keepdims=True)   \n",
    "\n",
    "    error_hidden = np.dot(error_output, W2.T) * sigmoid_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, error_hidden)\n",
    "    db2 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    error_hidden = np.dot(error_output, W1.T) * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, error_hidden)\n",
    "    db1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    W4 -= learning_rate * dW4\n",
    "    b4 -= learning_rate * db4\n",
    "    W5 -= learning_rate * dW5\n",
    "    b5 -= learning_rate * db5\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    z1, a1, z2, a2, z3, a3, z4, a4, z5, a5 = forward(x_train)\n",
    "    loss = cross_entropy_loss(a5, y_train)\n",
    "    backward(x_train, y_train, z1, a1, z2, a2, z3, a3, z4, a4, z5, a5)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Testing accuracy\n",
    "_, _, _, _, _, _, _, _, _, a5_test = forward(x_test)\n",
    "predictions = np.argmax(a5_test, axis=1)\n",
    "accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3037\n",
      "Epoch 2/10, Loss: 2.3037\n",
      "Epoch 3/10, Loss: 2.3037\n",
      "Epoch 4/10, Loss: 2.3037\n",
      "Epoch 5/10, Loss: 2.3036\n",
      "Epoch 6/10, Loss: 2.3036\n",
      "Epoch 7/10, Loss: 2.3036\n",
      "Epoch 8/10, Loss: 2.3036\n",
      "Epoch 9/10, Loss: 2.3036\n",
      "Epoch 10/10, Loss: 2.3036\n",
      "Test Accuracy: 9.13%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to integers and one-hot encode them\n",
    "y_train = y_train.astype(int).to_numpy()\n",
    "y_test = y_test.astype(int).to_numpy()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Convert data to NumPy arrays before creating PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train_tensor)\n",
    "    loss = criterion(outputs, torch.argmax(y_train_tensor, dim=1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing accuracy\n",
    "with torch.no_grad():\n",
    "    outputs_test = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs_test, 1)\n",
    "    accuracy = (predicted == torch.argmax(y_test_tensor, dim=1)).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.5652\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m z1, a1, z2, a2 \u001b[38;5;241m=\u001b[39m forward(x_train)\n\u001b[1;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(a2, y_train)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mforward_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 88\u001b[0m, in \u001b[0;36mforward_gradient_descent\u001b[0;34m(X, y, z1, a1, z2, a2)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(W1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     87\u001b[0m     W1[i, j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon\n\u001b[0;32m---> 88\u001b[0m     _, a1_perturbed, _, a2_perturbed \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     loss_perturbed \u001b[38;5;241m=\u001b[39m cross_entropy_loss(a2_perturbed, y)\n\u001b[1;32m     90\u001b[0m     dW1[i, j] \u001b[38;5;241m=\u001b[39m (loss_perturbed \u001b[38;5;241m-\u001b[39m cross_entropy_loss(a2, y)) \u001b[38;5;241m/\u001b[39m epsilon\n",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(X):\n\u001b[0;32m---> 52\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     53\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m sigmoid(z1)\n\u001b[1;32m     54\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a1, W2) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST data from sklearn's openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to integers and one-hot encode them\n",
    "y_train = y_train.astype(int).to_numpy()  # Convert to NumPy array\n",
    "y_test = y_test.astype(int).to_numpy()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Neural network parameters\n",
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "epsilon = 1e-4  # Small value for finite differences\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "# Forward Gradient Descent\n",
    "def forward_gradient_descent(X, y, z1, a1, z2, a2):\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    # Gradients for W2 and b2 using finite differences\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    db2 = np.zeros_like(b2)\n",
    "\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2[i, j] += epsilon\n",
    "            _, _, _, a2_perturbed = forward(X)\n",
    "            loss_perturbed = cross_entropy_loss(a2_perturbed, y)\n",
    "            dW2[i, j] = (loss_perturbed - cross_entropy_loss(a2, y)) / epsilon\n",
    "            W2[i, j] -= epsilon\n",
    "\n",
    "    for j in range(b2.shape[1]):\n",
    "        b2[0, j] += epsilon\n",
    "        _, _, _, a2_perturbed = forward(X)\n",
    "        loss_perturbed = cross_entropy_loss(a2_perturbed, y)\n",
    "        db2[0, j] = (loss_perturbed - cross_entropy_loss(a2, y)) / epsilon\n",
    "        b2[0, j] -= epsilon\n",
    "\n",
    "    # Gradients for W1 and b1\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    db1 = np.zeros_like(b1)\n",
    "\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1[i, j] += epsilon\n",
    "            _, a1_perturbed, _, a2_perturbed = forward(X)\n",
    "            loss_perturbed = cross_entropy_loss(a2_perturbed, y)\n",
    "            dW1[i, j] = (loss_perturbed - cross_entropy_loss(a2, y)) / epsilon\n",
    "            W1[i, j] -= epsilon\n",
    "\n",
    "    for j in range(b1.shape[1]):\n",
    "        b1[0, j] += epsilon\n",
    "        _, a1_perturbed, _, a2_perturbed = forward(X)\n",
    "        loss_perturbed = cross_entropy_loss(a2_perturbed, y)\n",
    "        db1[0, j] = (loss_perturbed - cross_entropy_loss(a2, y)) / epsilon\n",
    "        b1[0, j] -= epsilon\n",
    "\n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# Training loop with forward gradient descent\n",
    "for epoch in range(epochs):\n",
    "    z1, a1, z2, a2 = forward(x_train)\n",
    "    loss = cross_entropy_loss(a2, y_train)\n",
    "    forward_gradient_descent(x_train, y_train, z1, a1, z2, a2)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Testing accuracy\n",
    "_, _, _, a2_test = forward(x_train)\n",
    "predictions = np.argmax(a2_test, axis=1)\n",
    "accuracy = np.mean(predictions == np.argmax(y_train, axis=1))\n",
    "print(f\"Training Accuracy on Synthetic Data: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.6984\n",
      "Epoch 2/10, Loss: 2.6815\n",
      "Epoch 3/10, Loss: 2.6654\n",
      "Epoch 4/10, Loss: 2.6501\n",
      "Epoch 5/10, Loss: 2.6354\n",
      "Epoch 6/10, Loss: 2.6214\n",
      "Epoch 7/10, Loss: 2.6080\n",
      "Epoch 8/10, Loss: 2.5952\n",
      "Epoch 9/10, Loss: 2.5829\n",
      "Epoch 10/10, Loss: 2.5711\n",
      "Training Accuracy on Synthetic Data: 9.19%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST data from sklearn's openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to integers and one-hot encode them\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.astype(int).to_numpy()  # Convert to NumPy array\n",
    "y_test = y_test.astype(int).to_numpy()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Neural network parameters\n",
    "input_size = 784\n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))\n",
    "\n",
    "# Forward-mode AD function to calculate gradients directly in the forward pass\n",
    "def forward_ad(X):\n",
    "    global W1, b1, W2, b2\n",
    "\n",
    "    # Forward pass for values\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    # Forward-mode AD for gradients\n",
    "    # Initialize gradients with respect to each parameter\n",
    "    dW1 = np.zeros_like(W1)\n",
    "    db1 = np.zeros_like(b1)\n",
    "    dW2 = np.zeros_like(W2)\n",
    "    db2 = np.zeros_like(b2)\n",
    "    \n",
    "    # Compute gradients for W2 and b2\n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            # Perturb W2[i, j] and compute the gradient directly\n",
    "            dW2[i, j] = np.sum(a1[:, i] * (a2[:, j] - y_train[:, j])) / X.shape[0]\n",
    "    \n",
    "    db2[0, :] = np.sum(a2 - y_train, axis=0) / X.shape[0]\n",
    "    \n",
    "    # Compute gradients for W1 and b1\n",
    "    delta_hidden = (np.dot(a2 - y_train, W2.T) * sigmoid_derivative(a1))  # Backpropagated error to hidden layer\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            # Perturb W1[i, j] and compute the gradient directly\n",
    "            dW1[i, j] = np.sum(X[:, i] * delta_hidden[:, j]) / X.shape[0]\n",
    "    \n",
    "    db1[0, :] = np.sum(delta_hidden, axis=0) / X.shape[0]\n",
    "    \n",
    "    return a2, dW1, db1, dW2, db2\n",
    "\n",
    "# Training loop with forward-mode AD\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Perform forward pass and get gradients\n",
    "    a2, dW1, db1, dW2, db2 = forward_ad(x_train)\n",
    "    loss = cross_entropy_loss(a2, y_train)\n",
    "    \n",
    "    # Gradient descent update\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    # Print loss at each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test the training accuracy on synthetic data\n",
    "a2, _, _, _, _ = forward_ad(x_train)\n",
    "predictions = np.argmax(a2, axis=1)\n",
    "accuracy = np.mean(predictions == np.argmax(y_train, axis=1))\n",
    "print(f\"Training Accuracy on Synthetic Data: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.4263\n",
      "Epoch 2/10, Loss: 15.8614\n",
      "Epoch 3/10, Loss: 15.6920\n",
      "Epoch 4/10, Loss: 15.1998\n",
      "Epoch 5/10, Loss: 16.9301\n",
      "Epoch 6/10, Loss: 16.8340\n",
      "Epoch 7/10, Loss: 18.3289\n",
      "Epoch 8/10, Loss: 17.7703\n",
      "Epoch 9/10, Loss: 18.8570\n",
      "Epoch 10/10, Loss: 18.6998\n",
      "Training Accuracy on Synthetic Data: 9.79%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load MNIST data from sklearn's openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "x, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x = x / 255.0\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to integers and one-hot encode them\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_train = y_train.astype(int).to_numpy()  # Convert to NumPy array\n",
    "y_test = y_test.astype(int).to_numpy()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = one_hot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Neural network parameters\n",
    "input_size = 784\n",
    "h1_size = 512\n",
    "h2_size = 256\n",
    "h3_size = 128\n",
    "h4_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases with values and gradients (dual numbers)\n",
    "W1 = np.random.randn(input_size, h1_size) * np.sqrt(2. / input_size)\n",
    "b1 = np.zeros((1, h1_size))\n",
    "W2 = np.random.randn(h1_size, h2_size) * np.sqrt(2. / h1_size)\n",
    "b2 = np.zeros((1, h2_size))\n",
    "W3 = np.random.randn(h2_size, h3_size) * np.sqrt(2. / h2_size)\n",
    "b3 = np.zeros((1, h3_size))\n",
    "W4 = np.random.randn(h3_size, h4_size) * np.sqrt(2. / h3_size)\n",
    "b4 = np.zeros((1, h4_size))\n",
    "W5 = np.random.randn(h4_size, output_size) * np.sqrt(2. / h4_size)\n",
    "b5 = np.zeros((1, output_size))\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss function\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))\n",
    "\n",
    "# Forward-mode AD function\n",
    "def forward_ad(X):\n",
    "    global W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\n",
    "    # Forward pass for values and derivatives\n",
    "    \n",
    "    # Layer 1: Input to Hidden Layer\n",
    "    z1 = np.dot(X, W1) + b1  # regular forward pass\n",
    "    a1 = sigmoid(z1)         # activation\n",
    "\n",
    "    # Gradient of z1 w.r.t. W1, b1\n",
    "    dW1 = np.dot(X.T, sigmoid_derivative(a1))\n",
    "    db1 = np.sum(sigmoid_derivative(a1), axis=0, keepdims=True)\n",
    "    \n",
    "    # Layer 2: Hidden to Output Layer\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    dW2 = np.dot(a1.T, sigmoid_derivative(a2))\n",
    "    db2 = np.sum(sigmoid_derivative(a2), axis=0, keepdims=True)\n",
    "\n",
    "    # Layer 3: Hidden to Output Layer\n",
    "    z3 = np.dot(a2, W3) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    dW3 = np.dot(a2.T, sigmoid_derivative(a3))\n",
    "    db3 = np.sum(sigmoid_derivative(a3), axis=0, keepdims=True)\n",
    "\n",
    "    # Layer 4: Hidden to Output Layer\n",
    "    z4 = np.dot(a3, W4) + b4\n",
    "    a4 = sigmoid(z4)\n",
    "\n",
    "    dW4 = np.dot(a3.T, sigmoid_derivative(a4))\n",
    "    db4 = np.sum(sigmoid_derivative(a4), axis=0, keepdims=True)\n",
    "\n",
    "    # Layer 5: Hidden to Output Layer\n",
    "    z5 = np.dot(a4, W5) + b5\n",
    "    a5 = softmax(z5)\n",
    "\n",
    "    dW5 = np.dot(a4.T, (a5 - y_train))\n",
    "    db5 = np.sum(a5 - y_train, axis=0, keepdims=True)\n",
    "\n",
    "    return a5, dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5\n",
    "\n",
    "# Training loop with forward-mode AD\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # Perform forward pass and get gradients\n",
    "    a5, dW1, db1, dW2, db2, dW3, db3, dW4, db4, dW5, db5 = forward_ad(x_train)\n",
    "    loss = cross_entropy_loss(a5, y_train)\n",
    "    \n",
    "    # Gradient descent update\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    W4 -= learning_rate * dW4\n",
    "    b4 -= learning_rate * db4\n",
    "    W5 -= learning_rate * dW5\n",
    "    b5 -= learning_rate * db5\n",
    "    \n",
    "    # Print loss at each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test the training accuracy on synthetic data\n",
    "a5, _, _, _, _, _, _, _, _, _, _ = forward_ad(x_train)\n",
    "predictions = np.argmax(a5, axis=1)\n",
    "accuracy = np.mean(predictions == np.argmax(y_train, axis=1))\n",
    "print(f\"Training Accuracy on Synthetic Data: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking network with layer sizes: [128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_25034/1375182697.py:49: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  _, jvp_val = jvp(forward_fn, params, dummy_vec)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "During a grad (vjp, jvp, grad, etc) transform, the function provided attempted to call in-place operation (aten::copy_) that would mutate a captured Tensor. This is not supported; please rewrite the function being transformed to explicitly accept the mutated Tensor(s) as inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 102\u001b[0m\n\u001b[1;32m     94\u001b[0m layer_sizes_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     [\u001b[38;5;241m128\u001b[39m],         \u001b[38;5;66;03m# Single hidden layer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     [\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m],    \u001b[38;5;66;03m# Two hidden layers\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     [\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m],  \u001b[38;5;66;03m# Three hidden layers\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     [\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m]  \u001b[38;5;66;03m# Four hidden layers\u001b[39;00m\n\u001b[1;32m     99\u001b[0m ]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Run benchmarks\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_benchmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_sizes_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 76\u001b[0m, in \u001b[0;36mrun_benchmarks\u001b[0;34m(input_size, output_size, batch_size, layer_sizes_list, num_trials)\u001b[0m\n\u001b[1;32m     73\u001b[0m     backprop_times\u001b[38;5;241m.\u001b[39mappend(backprop_time)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Forward-mode AD\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     forward_ad_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_forward_ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     forward_ad_times\u001b[38;5;241m.\u001b[39mappend(forward_ad_time)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Store average times\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 49\u001b[0m, in \u001b[0;36mbenchmark_forward_ad\u001b[0;34m(model, input_data, target_data, criterion)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Forward pass and JVP (forward-mode AD)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 49\u001b[0m _, jvp_val \u001b[38;5;241m=\u001b[39m \u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_functorch/deprecated.py:96\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, primals, tangents, strict, has_aux)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(\n\u001b[1;32m     88\u001b[0m     func: Callable,\n\u001b[1;32m     89\u001b[0m     primals: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     has_aux: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     94\u001b[0m ):\n\u001b[1;32m     95\u001b[0m     warn_deprecated(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjvp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_functorch/eager_transforms.py:1084\u001b[0m, in \u001b[0;36mjvp\u001b[0;34m(func, primals, tangents, strict, has_aux)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;129m@exposed_in\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.func\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjvp\u001b[39m(\n\u001b[1;32m   1027\u001b[0m     func: Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     has_aux: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1033\u001b[0m ):\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    Standing for the Jacobian-vector product, returns a tuple containing\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    the output of `func(*primals)` and the \"Jacobian of ``func`` evaluated at\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \n\u001b[1;32m   1082\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_functorch/eager_transforms.py:1141\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m   1140\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m-> 1141\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[46], line 38\u001b[0m, in \u001b[0;36mbenchmark_forward_ad.<locals>.forward_fn\u001b[0;34m(*params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, p_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters(), params):\n\u001b[0;32m---> 38\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_data)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target_data)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: During a grad (vjp, jvp, grad, etc) transform, the function provided attempted to call in-place operation (aten::copy_) that would mutate a captured Tensor. This is not supported; please rewrite the function being transformed to explicitly accept the mutated Tensor(s) as inputs."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from functorch import jvp, make_functional\n",
    "\n",
    "# Function to create a feedforward neural network\n",
    "def create_network(input_size, layer_sizes, output_size):\n",
    "    layers = []\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(input_size, layer_size))\n",
    "        else:\n",
    "            layers.append(nn.Linear(layer_sizes[i - 1], layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Function to benchmark backpropagation\n",
    "def benchmark_backprop(model, input_data, target_data, criterion):\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target_data)\n",
    "    \n",
    "    # Backward pass\n",
    "    start_time = time.time()\n",
    "    loss.backward()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to benchmark forward-mode AD using jvp\n",
    "def benchmark_forward_ad(model, input_data, target_data, criterion):\n",
    "    # Define function for forward pass with unpacking\n",
    "    def forward_fn(*params):\n",
    "        # Assign parameters to model\n",
    "        with torch.no_grad():\n",
    "            for p, p_val in zip(model.parameters(), params):\n",
    "                p.data.copy_(p_val)\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target_data)\n",
    "        return loss\n",
    "\n",
    "    # Create dummy vector for jvp and convert params to tuple\n",
    "    params = tuple(model.parameters())\n",
    "    dummy_vec = tuple(torch.ones_like(p) for p in params)\n",
    "\n",
    "    # Forward pass and JVP (forward-mode AD)\n",
    "    start_time = time.time()\n",
    "    _, jvp_val = jvp(forward_fn, params, dummy_vec)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to run benchmarks for different network sizes\n",
    "def run_benchmarks(input_size, output_size, batch_size, layer_sizes_list, num_trials=5):\n",
    "    input_data = torch.randn(batch_size, input_size)\n",
    "    target_data = torch.randint(0, output_size, (batch_size,))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = []\n",
    "    for layer_sizes in layer_sizes_list:\n",
    "        print(f\"Benchmarking network with layer sizes: {layer_sizes}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = create_network(input_size, layer_sizes, output_size)\n",
    "        \n",
    "        # Run benchmarks\n",
    "        backprop_times = []\n",
    "        forward_ad_times = []\n",
    "        for _ in range(num_trials):\n",
    "            # Backpropagation\n",
    "            backprop_time = benchmark_backprop(model, input_data, target_data, criterion)\n",
    "            backprop_times.append(backprop_time)\n",
    "            \n",
    "            # Forward-mode AD\n",
    "            forward_ad_time = benchmark_forward_ad(model, input_data, target_data, criterion)\n",
    "            forward_ad_times.append(forward_ad_time)\n",
    "        \n",
    "        # Store average times\n",
    "        avg_backprop_time = sum(backprop_times) / num_trials\n",
    "        avg_forward_ad_time = sum(forward_ad_times) / num_trials\n",
    "        results.append({\n",
    "            \"Layer Sizes\": layer_sizes,\n",
    "            \"Backprop Time (s)\": avg_backprop_time,\n",
    "            \"Forward AD Time (s)\": avg_forward_ad_time\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Define network configurations to test\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "batch_size = 64\n",
    "layer_sizes_list = [\n",
    "    [128],         # Single hidden layer\n",
    "    [256, 128],    # Two hidden layers\n",
    "    [512, 256, 128],  # Three hidden layers\n",
    "    [512, 256, 128, 64]  # Four hidden layers\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "results = run_benchmarks(input_size, output_size, batch_size, layer_sizes_list)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking network with layer sizes: [128]\n",
      "Benchmarking network with layer sizes: [256, 128]\n",
      "Benchmarking network with layer sizes: [512, 256, 128]\n",
      "Benchmarking network with layer sizes: [512, 256, 128, 64]\n",
      "           Layer Sizes  Backprop Time (s)  Forward AD Time (s)\n",
      "0                [128]           0.000453             0.002594\n",
      "1           [256, 128]           0.000748             0.002718\n",
      "2      [512, 256, 128]           0.002067             0.005176\n",
      "3  [512, 256, 128, 64]           0.001675             0.003887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_25034/2525052501.py:34: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  fmodel, params = make_functional(model)\n",
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_25034/2525052501.py:47: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  _, jvp_val = jvp(forward_fn, (params,), (dummy_vec,))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from functorch import jvp, make_functional\n",
    "\n",
    "# Function to create a feedforward neural network\n",
    "def create_network(input_size, layer_sizes, output_size):\n",
    "    layers = []\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(input_size, layer_size))\n",
    "        else:\n",
    "            layers.append(nn.Linear(layer_sizes[i - 1], layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Function to benchmark backpropagation\n",
    "def benchmark_backprop(model, input_data, target_data, criterion):\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target_data)\n",
    "    \n",
    "    # Backward pass\n",
    "    start_time = time.time()\n",
    "    loss.backward()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to benchmark forward-mode AD using jvp\n",
    "def benchmark_forward_ad(model, input_data, target_data, criterion):\n",
    "    # Convert model to functional form\n",
    "    fmodel, params = make_functional(model)\n",
    "\n",
    "    # Define function for forward pass in functional form\n",
    "    def forward_fn(params):\n",
    "        output = fmodel(params, input_data)\n",
    "        loss = criterion(output, target_data)\n",
    "        return loss\n",
    "\n",
    "    # Create dummy vector for jvp\n",
    "    dummy_vec = tuple(torch.ones_like(p) for p in params)\n",
    "\n",
    "    # Forward pass and JVP (forward-mode AD)\n",
    "    start_time = time.time()\n",
    "    _, jvp_val = jvp(forward_fn, (params,), (dummy_vec,))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to run benchmarks for different network sizes\n",
    "def run_benchmarks(input_size, output_size, batch_size, layer_sizes_list, num_trials=5):\n",
    "    input_data = torch.randn(batch_size, input_size)\n",
    "    target_data = torch.randint(0, output_size, (batch_size,))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = []\n",
    "    for layer_sizes in layer_sizes_list:\n",
    "        print(f\"Benchmarking network with layer sizes: {layer_sizes}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = create_network(input_size, layer_sizes, output_size)\n",
    "        \n",
    "        # Run benchmarks\n",
    "        backprop_times = []\n",
    "        forward_ad_times = []\n",
    "        for _ in range(num_trials):\n",
    "            # Backpropagation\n",
    "            backprop_time = benchmark_backprop(model, input_data, target_data, criterion)\n",
    "            backprop_times.append(backprop_time)\n",
    "            \n",
    "            # Forward-mode AD\n",
    "            forward_ad_time = benchmark_forward_ad(model, input_data, target_data, criterion)\n",
    "            forward_ad_times.append(forward_ad_time)\n",
    "        \n",
    "        # Store average times\n",
    "        avg_backprop_time = sum(backprop_times) / num_trials\n",
    "        avg_forward_ad_time = sum(forward_ad_times) / num_trials\n",
    "        results.append({\n",
    "            \"Layer Sizes\": layer_sizes,\n",
    "            \"Backprop Time (s)\": avg_backprop_time,\n",
    "            \"Forward AD Time (s)\": avg_forward_ad_time\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Define network configurations to test\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "batch_size = 512\n",
    "layer_sizes_list = [\n",
    "    [128],         # Single hidden layer\n",
    "    [256, 128],    # Two hidden layers\n",
    "    [512, 256, 128],  # Three hidden layers\n",
    "    [512, 256, 128, 64]  # Four hidden layers\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "results = run_benchmarks(input_size, output_size, batch_size, layer_sizes_list)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking network with layer sizes: [128]\n",
      "Benchmarking network with layer sizes: [256, 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_25034/1272575570.py:38: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  fmodel, params = make_functional(model)\n",
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_25034/1272575570.py:51: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.jvp` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jvp` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  _, jvp_val = jvp(forward_fn, (params,), (dummy_vec,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking network with layer sizes: [512, 256, 128]\n",
      "Benchmarking network with layer sizes: [512, 256, 128, 64]\n",
      "           Layer Sizes  Backprop Time (s)  Forward AD Time (s)\n",
      "0                [128]           0.005485             0.014256\n",
      "1           [256, 128]           0.014681             0.033693\n",
      "2      [512, 256, 128]           0.023862             0.053527\n",
      "3  [512, 256, 128, 64]           0.023846             0.051046\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from functorch import jvp, make_functional\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to create a feedforward neural network\n",
    "def create_network(input_size, layer_sizes, output_size):\n",
    "    layers = []\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(input_size, layer_size))\n",
    "        else:\n",
    "            layers.append(nn.Linear(layer_sizes[i - 1], layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Function to benchmark backpropagation\n",
    "def benchmark_backprop(model, input_data, target_data, criterion):\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = criterion(output, target_data)\n",
    "    \n",
    "    # Backward pass\n",
    "    start_time = time.time()\n",
    "    loss.backward()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to benchmark forward-mode AD using jvp\n",
    "def benchmark_forward_ad(model, input_data, target_data, criterion):\n",
    "    # Convert model to functional form\n",
    "    fmodel, params = make_functional(model)\n",
    "\n",
    "    # Define function for forward pass in functional form\n",
    "    def forward_fn(params):\n",
    "        output = fmodel(params, input_data)\n",
    "        loss = criterion(output, target_data)\n",
    "        return loss\n",
    "\n",
    "    # Create dummy vector for jvp\n",
    "    dummy_vec = tuple(torch.ones_like(p) for p in params)\n",
    "\n",
    "    # Forward pass and JVP (forward-mode AD)\n",
    "    start_time = time.time()\n",
    "    _, jvp_val = jvp(forward_fn, (params,), (dummy_vec,))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to run benchmarks for different network sizes\n",
    "def run_benchmarks(input_data, target_data, input_size, output_size, layer_sizes_list, num_trials=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    results = []\n",
    "    for layer_sizes in layer_sizes_list:\n",
    "        print(f\"Benchmarking network with layer sizes: {layer_sizes}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = create_network(input_size, layer_sizes, output_size)\n",
    "        \n",
    "        # Run benchmarks\n",
    "        backprop_times = []\n",
    "        forward_ad_times = []\n",
    "        for _ in range(num_trials):\n",
    "            # Backpropagation\n",
    "            backprop_time = benchmark_backprop(model, input_data, target_data, criterion)\n",
    "            backprop_times.append(backprop_time)\n",
    "            \n",
    "            # Forward-mode AD\n",
    "            forward_ad_time = benchmark_forward_ad(model, input_data, target_data, criterion)\n",
    "            forward_ad_times.append(forward_ad_time)\n",
    "        \n",
    "        # Store average times\n",
    "        avg_backprop_time = sum(backprop_times) / num_trials\n",
    "        avg_forward_ad_time = sum(forward_ad_times) / num_trials\n",
    "        results.append({\n",
    "            \"Layer Sizes\": layer_sizes,\n",
    "            \"Backprop Time (s)\": avg_backprop_time,\n",
    "            \"Forward AD Time (s)\": avg_forward_ad_time\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def load_mnist_data():\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    x, y = mnist['data'], mnist['target']\n",
    "    x = x / 255.0  # Normalize to [0, 1]\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    x_train, _, y_train, _ = train_test_split(x, y, test_size=0.9, random_state=42)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# Define network configurations to test\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "layer_sizes_list = [\n",
    "    [128],         # Single hidden layer\n",
    "    [256, 128],    # Two hidden layers\n",
    "    [512, 256, 128],  # Three hidden layers\n",
    "    [512, 256, 128, 64]  # Four hidden layers\n",
    "]\n",
    "\n",
    "# Load MNIST data\n",
    "input_data, target_data = load_mnist_data()\n",
    "\n",
    "# Run benchmarks\n",
    "results = run_benchmarks(input_data, target_data, input_size, output_size, layer_sizes_list)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking network with layer sizes: [128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_45963/2836665056.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  fmodel, params = make_functional(model)\n",
      "/var/folders/2m/zt75k_js249d_qcmywz2vb0r0000gp/T/ipykernel_45963/2836665056.py:50: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.jacfwd` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jacfwd` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  grads = jacfwd(forward_fn)(params)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from functorch import jacfwd, make_functional\n",
    "from torch.utils._pytree import tree_flatten, tree_unflatten\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to create a feedforward neural network\n",
    "def create_network(input_size, layer_sizes, output_size):\n",
    "    layers = []\n",
    "    for i, layer_size in enumerate(layer_sizes):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Linear(input_size, layer_size))\n",
    "        else:\n",
    "            layers.append(nn.Linear(layer_sizes[i - 1], layer_size))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.Linear(layer_sizes[-1], output_size))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Function to train with backpropagation\n",
    "def train_backprop(model, input_data, target_data, criterion, optimizer, epochs):\n",
    "    start_time = time.time()\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to train with forward-mode AD using jvp\n",
    "def train_forward_ad(model, input_data, target_data, criterion, epochs, learning_rate):\n",
    "    # Convert model to functional form\n",
    "    fmodel, params = make_functional(model)\n",
    "\n",
    "    def forward_fn(params):\n",
    "        output = fmodel(params, input_data)\n",
    "        loss = criterion(output, target_data)\n",
    "        return loss  # return a scalar loss\n",
    "\n",
    "    # Flatten params to avoid nested structure issues\n",
    "    flat_params, params_spec = tree_flatten(params)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for _ in range(epochs):\n",
    "        # Compute gradients for forward-mode AD\n",
    "        grads = jacfwd(forward_fn)(params)\n",
    "\n",
    "        # Flatten gradients to match flat_params\n",
    "        flat_grads, _ = tree_flatten(grads)\n",
    "\n",
    "        # Update parameters (flat updates)\n",
    "        flat_params = [p - learning_rate * g for p, g in zip(flat_params, flat_grads)]\n",
    "\n",
    "        # Reconstruct parameter structure\n",
    "        params = tree_unflatten(flat_params, params_spec)\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to run benchmarks for different network sizes\n",
    "def run_benchmarks(input_data, target_data, input_size, output_size, layer_sizes_list, epochs, num_trials=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    results = []\n",
    "\n",
    "    for layer_sizes in layer_sizes_list:\n",
    "        print(f\"Benchmarking network with layer sizes: {layer_sizes}\")\n",
    "        \n",
    "        # Initialize model\n",
    "        model = create_network(input_size, layer_sizes, output_size)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Run benchmarks\n",
    "        backprop_times = []\n",
    "        forward_ad_times = []\n",
    "        for _ in range(num_trials):\n",
    "            # Backpropagation training\n",
    "            backprop_time = train_backprop(model, input_data, target_data, criterion, optimizer, epochs)\n",
    "            backprop_times.append(backprop_time)\n",
    "            \n",
    "            # Forward-mode AD training\n",
    "            forward_ad_time = train_forward_ad(model, input_data, target_data, criterion, epochs, learning_rate)\n",
    "            forward_ad_times.append(forward_ad_time)\n",
    "        \n",
    "        # Store average times\n",
    "        avg_backprop_time = sum(backprop_times) / num_trials\n",
    "        avg_forward_ad_time = sum(forward_ad_times) / num_trials\n",
    "        results.append({\n",
    "            \"Layer Sizes\": layer_sizes,\n",
    "            \"Backprop Total Time (s)\": avg_backprop_time,\n",
    "            \"Forward AD Total Time (s)\": avg_forward_ad_time,\n",
    "            \"Avg Backprop Epoch Time (s)\": avg_backprop_time / epochs,\n",
    "            \"Avg Forward AD Epoch Time (s)\": avg_forward_ad_time / epochs\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "def load_mnist_data():\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    x, y = mnist['data'], mnist['target']\n",
    "    x = x / 255.0  # Normalize to [0, 1]\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    x_train, _, y_train, _ = train_test_split(x, y, test_size=0.9, random_state=42)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x_train = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# Define network configurations to test\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "layer_sizes_list = [\n",
    "    [128],         # Single hidden layer\n",
    "    [256, 128],    # Two hidden layers\n",
    "    [512, 256, 128],  # Three hidden layers\n",
    "    [512, 256, 128, 64]  # Four hidden layers\n",
    "]\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 5  # Adjust as needed for longer training\n",
    "\n",
    "# Load MNIST data\n",
    "input_data, target_data = load_mnist_data()\n",
    "\n",
    "# Run benchmarks\n",
    "results = run_benchmarks(input_data, target_data, input_size, output_size, layer_sizes_list, epochs)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
