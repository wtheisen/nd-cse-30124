{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1962f76a",
   "metadata": {},
   "source": [
    "![Slide 1 - Decision Trees Title](../img/lecture06_decision_trees/slide-01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4905e4",
   "metadata": {},
   "source": [
    "# Lecture 06 \u2014 Decision Trees (Instructor Notes)\n",
    "\n",
    "Welcome! This notebook is my spoken commentary in text form. It pairs each slide with a gentle explanation and small code steps that you can run and remix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c70e6e",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "- Read the short explanation under each slide image, then run the tiny code block that follows.\n",
    "- I keep code cells small and commented so the flow is easy to follow.\n",
    "- Mathematical ideas appear as clean equations using MathJax; feel free to click \u201cEdit\u201d on any Markdown cell to see the raw notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02ba51",
   "metadata": {},
   "source": [
    "![Slide 3 - Learning to Generalize](../img/lecture06_decision_trees/slide-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb79828",
   "metadata": {},
   "source": [
    "## Slide 3 \u2014 Learning to Generalize\n",
    "We train models so they make good predictions on **new** examples, not just the ones we showed them. Decision trees do this by asking a sequence of simple questions and following the answers down to a leaf.\n",
    "\n",
    "Key terms:\n",
    "- **Features**: observable facts about an example (e.g., color, temperature).\n",
    "- **Label**: the answer to predict (today: the piece's shape).\n",
    "- **Model**: a rule for mapping features \u2192 label (a tree of questions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848679f",
   "metadata": {},
   "source": [
    "### The Pipeline, Demystified\n",
    "The \u201cdata in \u2192 answers out\u201d diagram on the slide hides the rule-making step. Here, that rule is a tree whose internal nodes are questions and whose leaves are predictions. We\u2019ll start with a tiny, tactile example before we scale up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a92cc",
   "metadata": {},
   "source": [
    "![Slide 6 - Entropy and Information Gain](../img/lecture06_decision_trees/slide-06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f50ef",
   "metadata": {},
   "source": [
    "## Slide 6 \u2014 Entropy and Information Gain\n",
    "Intuitively, **entropy** measures how mixed the labels are (how uncertain we are). If nearly every piece is a cube, there\u2019s little surprise when we draw; if many shapes are equally likely, surprise is high.\n",
    "\n",
    "Mathematically, for a random label \\(Y\\) with class probabilities \\(p_1, p_2, \\dots, p_k\\), the (Shannon) entropy in bits is\n",
    "\n",
    "$$\n",
    "H(Y) = -\\sum_{i=1}^{k} p_i \\, \\log_2 p_i.\n",
    "$$\n",
    "\n",
    "Two anchors for your intuition:\n",
    "- Zero entropy when one outcome has probability 1.\n",
    "- Maximum entropy when all outcomes are equally likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db90b66b",
   "metadata": {},
   "source": [
    "### Why Entropy Matters for Trees\n",
    "At each node, a tree chooses the question that most reduces uncertainty about the label. That reduction is called **information gain**. Given a feature \\(A\\) with possible values \\(v\\), the conditional entropy is\n",
    "\n",
    "$$\n",
    "H(Y\\mid A) = \\sum_{v} \\Pr(A{=}v)\\, H\bigl(Y \\mid A{=}v\bigr),\n",
    "$$\n",
    "\n",
    "and the information gain of splitting on \\(A\\) is\n",
    "\n",
    "$$\n",
    "\\operatorname{IG}(Y; A) = H(Y) - H(Y\\mid A).\n",
    "$$\n",
    "\n",
    "We will compute these quantities by hand first, then let a library do the same job for bigger datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d6eef",
   "metadata": {},
   "source": [
    "### Imports We Reuse\n",
    "We\u2019ll use pandas for tables, NumPy for arrays, matplotlib for plots, and scikit-learn for trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Make plots easy to read\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff97e0",
   "metadata": {},
   "source": [
    "### Building Up Entropy Gently\n",
    "We\u2019ll start from probabilities, then examine special cases, and finally translate the same formula into reusable functions that accept labels or counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81547af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy given probabilities; normalizes if raw counts are supplied\n",
    "# H(p) = - sum_i p_i log2 p_i\n",
    "\n",
    "def entropy_from_probabilities(probabilities):\n",
    "    total = sum(probabilities)\n",
    "    if not math.isclose(total, 1.0):\n",
    "        probabilities = [p / total for p in probabilities]\n",
    "    return -sum(p * math.log2(p) for p in probabilities if p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07f6d8",
   "metadata": {},
   "source": [
    "Let\u2019s sweep the binary case (cube vs. coin) to see the classic dome-shaped curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de267c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy as the probability of \"cube\" varies from 0 to 1\n",
    "p_values = np.linspace(0.0, 1.0, 201)\n",
    "H_binary = [entropy_from_probabilities([p, 1 - p]) for p in p_values]\n",
    "\n",
    "plt.figure(figsize=(6.2, 4.2))\n",
    "plt.plot(p_values, H_binary, color='navy')\n",
    "plt.xlabel('P(cube)')\n",
    "plt.ylabel('Entropy H(Y) [bits]')\n",
    "plt.title('Binary Entropy: smallest at certainty, largest at 50/50')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe24777",
   "metadata": {},
   "source": [
    "A few numeric checkpoints for the binary case:\n",
    "- \\(H(0) = H(1) = 0\\) (no surprise when the outcome is certain).\n",
    "- \\(H(0.5) = 1\\) bit (the most uncertain, equally mixed case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682d70b",
   "metadata": {},
   "source": [
    "### From Probabilities to Data\n",
    "Now we\u2019ll turn the same idea into helpers that accept labels or raw counts directly:\n",
    "\n",
    "- Given counts \\((c_1, \\dots, c_k)\\), we use \\(p_i = c_i / \\sum_j c_j\\).\n",
    "- Given labels, we count them first and then apply the same formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b831fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy from class counts\n",
    "# H = - sum_i (c_i / N) log2 (c_i / N)\n",
    "\n",
    "def entropy_from_counts(counts):\n",
    "    total = sum(counts)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return -sum((c / total) * math.log2(c / total) for c in counts if c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy from a list/Series of labels\n",
    "def entropy(labels):\n",
    "    return entropy_from_counts(Counter(labels).values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6db2d",
   "metadata": {},
   "source": [
    "### Quick Sanity Checks\n",
    "Pure vs. perfectly mixed bags should give \\(0\\) and \\(1\\) bit respectively in the two-class case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: certain outcome (entropy 0)\n",
    "all_cubes = ['cube'] * 4\n",
    "zero_entropy = entropy(all_cubes)\n",
    "\n",
    "# Scenario 2: equally mixed (entropy 1 bit)\n",
    "mixed = ['cube', 'cube', 'coin', 'coin']\n",
    "one_bit = entropy(mixed)\n",
    "\n",
    "zero_entropy, one_bit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c259",
   "metadata": {},
   "source": [
    "### Conditional Entropy and Information Gain\n",
    "Given a candidate question (feature \\(A\\)), we measure how much uncertainty remains after splitting on it:\n",
    "\n",
    "$$\n",
    "H(Y\\mid A) = \\sum_{v} \\Pr(A{=}v)\\, H\bigl(Y \\mid A{=}v\bigr), \\qquad\n",
    "\\operatorname{IG}(Y; A) = H(Y) - H(Y\\mid A).\n",
    "$$\n",
    "\n",
    "We\u2019ll implement exactly these two expressions in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional entropy H(Y|A) computed by grouping on feature values\n",
    "def conditional_entropy(feature, target):\n",
    "    total = len(target)\n",
    "    groups = {}\n",
    "    for f, y in zip(feature, target):\n",
    "        groups.setdefault(f, []).append(y)\n",
    "    return sum(len(vals) / total * entropy(vals) for vals in groups.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a41b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information gain: how much splitting on A reduces uncertainty in Y\n",
    "def information_gain(feature, target):\n",
    "    return entropy(target) - conditional_entropy(feature, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac3339",
   "metadata": {},
   "source": [
    "![Slide 8 - Board Game Inventory](../img/lecture06_decision_trees/slide-08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b45ad",
   "metadata": {},
   "source": [
    "## Slide 8 \u2014 Board Game Inventory (20-piece Bag)\n",
    "We recorded each piece\u2019s color, material, silhouette, and the true shape we want to predict. Our bag:\n",
    "\n",
    "- 5 yellow cubes \u00b7 2 yellow coins \u00b7 2 red cubes \u00b7 2 red ships\n",
    "- 1 red meeple \u00b7 3 red soldiers \u00b7 3 brown cubes \u00b7 1 brown pawn \u00b7 1 green cube\n",
    "\n",
    "Our goal is to predict the **shape** from observed features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31272a",
   "metadata": {},
   "source": [
    "### Building the Table\n",
    "Each tuple below is `(color, material, silhouette, shape, count)`. We expand counts to one row per piece so the table mirrors the physical bag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009266f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_specs = [\n",
    "    ('yellow', 'wood',   'blocky',    'cube',    5),\n",
    "    ('yellow', 'metal',  'flat',      'coin',    2),\n",
    "    ('red',    'wood',   'blocky',    'cube',    2),\n",
    "    ('red',    'plastic','long',      'ship',    2),\n",
    "    ('red',    'wood',   'character', 'meeple',  1),\n",
    "    ('red',    'plastic','character', 'soldier', 3),\n",
    "    ('brown',  'wood',   'blocky',    'cube',    3),\n",
    "    ('brown',  'plastic','token',     'pawn',    1),\n",
    "    ('green',  'wood',   'blocky',    'cube',    1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for color, material, silhouette, shape, count in inventory_specs:\n",
    "    rows.extend({\n",
    "        'color': color,\n",
    "        'material': material,\n",
    "        'silhouette': silhouette,\n",
    "        'shape': shape,\n",
    "    } for _ in range(count))\n",
    "\n",
    "pieces = pd.DataFrame(rows)\n",
    "pieces.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848aaa9f",
   "metadata": {},
   "source": [
    "### How Uncertain Are We, Initially?\n",
    "Compute the label mix and its entropy/gini. Gini impurity is an alternative to entropy used by CART:\n",
    "\n",
    "$$\n",
    "\\operatorname{Gini}(Y) = 1 - \\sum_{i} p_i^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_counts = pieces['shape'].value_counts()\n",
    "\n",
    "# Gini impurity helper\n",
    "def gini(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels).values()\n",
    "    return 1.0 - sum((c/total)**2 for c in counts)\n",
    "\n",
    "bag_entropy = entropy(pieces['shape'])\n",
    "bag_gini = gini(pieces['shape'])\n",
    "\n",
    "shape_counts, bag_entropy, bag_gini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358b35f",
   "metadata": {},
   "source": [
    "![Slide 10 - Evaluating Splits](../img/lecture06_decision_trees/slide-10.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8944a",
   "metadata": {},
   "source": [
    "## Slide 10 \u2014 Evaluating Candidate Splits\n",
    "At a node we pick the question with the largest information gain (or, equivalently, the largest decrease in impurity). We\u2019ll compare three features: color, material, silhouette.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc715f1",
   "metadata": {},
   "source": [
    "### Compute IG and Post-split Gini\n",
    "We\u2019ll compute both to show they tell a consistent story on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['color', 'material', 'silhouette']\n",
    "records = []\n",
    "for feat in features:\n",
    "    ig = information_gain(pieces[feat], pieces['shape'])\n",
    "    gini_after = sum(\n",
    "        len(group)/len(pieces) * gini(group)\n",
    "        for _, group in pieces.groupby(feat)['shape']\n",
    "    )\n",
    "    records.append({'feature': feat, 'information_gain': ig, 'weighted_gini': gini_after})\n",
    "\n",
    "pd.DataFrame(records).sort_values('information_gain', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7a546",
   "metadata": {},
   "source": [
    "Silhouette typically wins\u2014it separates flat tokens from blocky cubes and character figures. Color and material help refine the remaining uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5baa57d",
   "metadata": {},
   "source": [
    "![Slide 12 - First Levels of the Tree](../img/lecture06_decision_trees/slide-12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f70e7",
   "metadata": {},
   "source": [
    "## Slide 12 \u2014 First Levels of the Tree\n",
    "Let\u2019s fit a small tree (depth 2) so the picture stays readable and aligns with the slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a53c2",
   "metadata": {},
   "source": [
    "### One-Hot Encoding and Fit\n",
    "Scikit-learn expects numeric features. `pandas.get_dummies` turns categories into 0/1 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9661544",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = pd.get_dummies(pieces[['color','material','silhouette']])\n",
    "y_small = pieces['shape']\n",
    "\n",
    "clf_small = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=42)\n",
    "clf_small.fit(X_small, y_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(\n",
    "    clf_small,\n",
    "    feature_names=X_small.columns,\n",
    "    class_names=clf_small.classes_,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    impurity=True,\n",
    ")\n",
    "plt.title('Depth-2 Tree Trained on the 20-piece Bag')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91edf646",
   "metadata": {},
   "source": [
    "Notice how the root split mirrors the information-gain ranking. Leaves display class distributions and node impurity to make the connection to our formulas explicit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43257999",
   "metadata": {},
   "source": [
    "![Slide 15 - Play Tennis Dataset](../img/lecture06_decision_trees/slide-15.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75dd03",
   "metadata": {},
   "source": [
    "## Slide 15 \u2014 Play Tennis Example\n",
    "A classic dataset to practice attribute selection. We\u2019ll rebuild a tiny table and recompute entropy and information gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_tennis_rows = [\n",
    "    ('Sunny', 'Hot', 'High', 'Weak',   'No'),\n",
    "    ('Sunny', 'Hot', 'High', 'Strong', 'No'),\n",
    "    ('Overcast', 'Hot', 'High', 'Weak','Yes'),\n",
    "    ('Rain', 'Mild', 'High', 'Weak',   'Yes'),\n",
    "    ('Rain', 'Cool', 'Normal','Weak',  'Yes'),\n",
    "    ('Rain', 'Cool', 'Normal','Strong','No'),\n",
    "    ('Overcast','Cool','Normal','Strong','Yes'),\n",
    "    ('Sunny', 'Mild', 'High', 'Weak',   'No'),\n",
    "    ('Sunny', 'Cool', 'Normal','Weak',  'Yes'),\n",
    "    ('Rain', 'Mild', 'Normal','Weak',   'Yes'),\n",
    "    ('Sunny', 'Mild', 'Normal','Strong','Yes'),\n",
    "    ('Overcast','Mild','High','Strong', 'Yes'),\n",
    "    ('Overcast','Hot', 'Normal','Weak', 'Yes'),\n",
    "    ('Rain', 'Mild', 'High', 'Strong',  'No'),\n",
    "]\n",
    "play_tennis = pd.DataFrame(play_tennis_rows, columns=['Outlook','Temperature','Humidity','Wind','Play'])\n",
    "play_tennis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e300ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_play = entropy(play_tennis['Play'])\n",
    "IGs = {col: information_gain(play_tennis[col], play_tennis['Play'])\n",
    "       for col in ['Outlook','Temperature','Humidity','Wind']}\n",
    "H_play, IGs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d7174",
   "metadata": {},
   "source": [
    "`Outlook` has the largest information gain, which is why it appears at the root of hand-built trees for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22ac04",
   "metadata": {},
   "source": [
    "![Slide 18 - From Calculations to Code](../img/lecture06_decision_trees/slide-18.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ede6a",
   "metadata": {},
   "source": [
    "## Slide 18 \u2014 From Hand Calculations to Tooling\n",
    "Let scikit-learn automate the same choices once we one-hot encode the categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b182ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_play = pd.get_dummies(play_tennis[['Outlook','Temperature','Humidity','Wind']])\n",
    "y_play = play_tennis['Play']\n",
    "\n",
    "clf_play = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "clf_play.fit(X_play, y_play)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755760f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plot_tree(clf_play, feature_names=X_play.columns, class_names=clf_play.classes_, filled=True, rounded=True)\n",
    "plt.title('Decision Tree for Play Tennis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a4258",
   "metadata": {},
   "source": [
    "![Slide 20 - Scaling Up with Iris](../img/lecture06_decision_trees/slide-20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a0c29",
   "metadata": {},
   "source": [
    "## Slide 20 \u2014 Scaling Up with Iris\n",
    "We\u2019ll compare training vs. test accuracy to see generalization in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b00bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=7)\n",
    "\n",
    "clf_iris = DecisionTreeClassifier(random_state=7)\n",
    "clf_iris.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, clf_iris.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, clf_iris.predict(X_test))\n",
    "train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53079e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 7))\n",
    "plot_tree(clf_iris, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True)\n",
    "plt.title('Decision Tree Trained on Iris')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfe7c0",
   "metadata": {},
   "source": [
    "![Slide 22 - Controlling Depth](../img/lecture06_decision_trees/slide-22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156bcb8",
   "metadata": {},
   "source": [
    "## Slide 22 \u2014 Controlling Depth and Overfitting\n",
    "As depth increases, training accuracy rises monotonically, but test accuracy typically peaks and then falls. We\u2019ll sweep `max_depth`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = range(1, 11)\n",
    "train_scores, test_scores = [], []\n",
    "for d in max_depths:\n",
    "    m = DecisionTreeClassifier(max_depth=d, random_state=7)\n",
    "    m.fit(X_train, y_train)\n",
    "    train_scores.append(m.score(X_train, y_train))\n",
    "    test_scores.append(m.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(max_depths, train_scores, marker='o', label='Train')\n",
    "plt.plot(max_depths, test_scores, marker='s', label='Test')\n",
    "plt.xlabel('Max depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Bias\u2013Variance Trade-off via Tree Depth (Iris)')\n",
    "plt.ylim(0.7, 1.05)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711728b8",
   "metadata": {},
   "source": [
    "![Slide 24 - Regression Trees](../img/lecture06_decision_trees/slide-24.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef5d11",
   "metadata": {},
   "source": [
    "## Slide 24 \u2014 Regression Trees and Variance Reduction\n",
    "In regression, we minimize **variance** within nodes rather than entropy/gini. For a real-valued target \\(Y\\), node impurity can be measured as\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(Y) = \frac{1}{n}\\sum_{i=1}^n (y_i - \bar y)^2.\n",
    "$$\n",
    "\n",
    "We\u2019ll see how depth affects error on the diabetes dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebfaaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes(as_frame=True)\n",
    "Xr = diabetes.data\n",
    "yr = diabetes.target\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.3, random_state=11)\n",
    "\n",
    "results = []\n",
    "for depth in [1, 2, 4, 6, None]:\n",
    "    reg = DecisionTreeRegressor(max_depth=depth, random_state=11)\n",
    "    reg.fit(Xr_train, yr_train)\n",
    "    rmse_train = mean_squared_error(yr_train, reg.predict(Xr_train), squared=False)\n",
    "    rmse_test  = mean_squared_error(yr_test,  reg.predict(Xr_test),  squared=False)\n",
    "    results.append({'max_depth': depth if depth is not None else 'full', 'rmse_train': rmse_train, 'rmse_test': rmse_test})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea6a18",
   "metadata": {},
   "source": [
    "![Slide 26 - Key Takeaways](../img/lecture06_decision_trees/slide-26.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b789e2a",
   "metadata": {},
   "source": [
    "## Slide 26 \u2014 Key Takeaways\n",
    "- Entropy and gini quantify \u201chow mixed\u201d labels are; information gain is the drop in that uncertainty when we split.\n",
    "- Trees are interpretable because questions are human-readable; impurity numbers connect directly to the math.\n",
    "- Depth and related knobs (min samples per leaf, etc.) regularize the model to avoid overfitting.\n",
    "- The same framework extends to regression by swapping the impurity measure for variance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}