{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 5: NLP - RNN and Transformer\n",
    "1. Given a dataset of known ciphers\n",
    "2. Match letter cipher text to collection of known cipher words\n",
    "3. Using the known cipher, generate a new training dataset\n",
    "4. Implement an RNN using only numpy\n",
    "5. Train the RNN to decipher text\n",
    "6. Train a seq2seq transformer to go from ciphertext to plaintext\n",
    "7. Use the transformer to decode the secret message\n",
    "8. Using the padlock code find the key evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letter Segmentation and OCR\n",
    "\n",
    "Thanks to your work on homework03 and homework04, the police were able to fully extract the text from the kidnapping letters:\n",
    "\n",
    "```\"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"``` \n",
    "\n",
    "*(This is not the actual text, the original cipher I chose to encode the kidnapping letters ended up being way too hard for a seq2seq model to break, which is great for the field of cryptography but kind of embarrassing for AI)*\n",
    "\n",
    "Unfortunately, it appears that the text has been encoded somehow! As you try to recover from pigtostal (swimming lost the houses for that right?) you realize that maybe you could treat this as a machine translation task. You could pretend the cipher is the source language and the plaintext is the target language.\n",
    "\n",
    "If you could figure out the cipher then you could generate a training set of (cipher, plaintext) pairs. You could then train a seq2seq model to translate the ciphertext into plaintext!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_text = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Similarity\n",
    "\n",
    "Your first step is to figure out which cipher was most likely used to encode the kidnapping letters. To do this, you should test a Bag of Words and a Bag of Characters similarity metric. As with most of machine learning, we need to somehow convert our text into a vector to allow us to compare it to other vectors. One option would be to use a pre-trained embedding network like word2vec or GloVe (this would be like using the output of convolutional layers as features for MNIST). However, for now we will just use a Bag of Words and an N-Gram model instead (much simpler).\n",
    "\n",
    "##### Bag of Words\n",
    "\n",
    "![Bag of Words](https://datascientyst.com/content/images/2023/01/bag_of_words_python.webp)\n",
    "\n",
    "##### N-Grams\n",
    "\n",
    "If you can remember all the way back to lecture07: Markov Models, you'll remember that we've actually seen these before, in the context of a Markov Babbler!\n",
    "\n",
    "![N-Grams](https://studymachinelearning.com/wp-content/uploads/2019/09/n-gram_ex1.png)\n",
    "\n",
    "However for this task, I'd recommend using a character level N-Gram model instead, as we just want to compare character frequencies, it's tough to predict how ciphers will modify words and there's unlikely to be any overlap between the two.\n",
    "\n",
    "##### Ciphers\n",
    "\n",
    "Your task will be to use the following ciphers to encode some basic text, and then implement BoW and character level N-Gram similarity functions to try and figure out which cipher was most likely used to encode the kidnapping letters.\n",
    "\n",
    "The ciphers to test are: Caesar, Vigenère, Substitution, Affine, and ROT13.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------\n",
    "# CIPHER IMPLEMENTATIONS\n",
    "# -------------------------------\n",
    "\n",
    "def caesar_cipher(text, shift):\n",
    "    \"\"\"Encode text using a Caesar cipher with the specified shift.\"\"\"\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            base = 'A' if char.isupper() else 'a'\n",
    "            result += chr((ord(char) - ord(base) + shift) % 26 + ord(base))\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def rot13(text):\n",
    "    \"\"\"Encode text using ROT13 (a Caesar cipher with shift 13).\"\"\"\n",
    "    return caesar_cipher(text, 13)\n",
    "\n",
    "def vigenere_cipher(text, keyword):\n",
    "    \"\"\"Encode text using the Vigenère cipher with a given keyword.\"\"\"\n",
    "    result = \"\"\n",
    "    keyword = keyword.lower()\n",
    "    keyword_length = len(keyword)\n",
    "    j = 0  # index for keyword letters\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            base = 'A' if char.isupper() else 'a'\n",
    "            # Determine shift amount from the keyword character\n",
    "            shift = ord(keyword[j % keyword_length]) - ord('a')\n",
    "            result += chr((ord(char) - ord(base) + shift) % 26 + ord(base))\n",
    "            j += 1\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def substitution_cipher(text, mapping):\n",
    "    \"\"\"\n",
    "    Encode text using a substitution cipher defined by the given mapping.\n",
    "    'mapping' should be a dictionary mapping each lowercase letter to its substitute.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            if char.isupper():\n",
    "                # Convert to lower case, substitute, then convert back to upper case\n",
    "                sub = mapping.get(char.lower(), char.lower()).upper()\n",
    "            else:\n",
    "                sub = mapping.get(char, char)\n",
    "            result += sub\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "def affine_cipher(text, a, b):\n",
    "    \"\"\"\n",
    "    Encode text using an Affine cipher with parameters a and b.\n",
    "    The function assumes that a and 26 are coprime.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            base = 'A' if char.isupper() else 'a'\n",
    "            x = ord(char) - ord(base)\n",
    "            # Apply the affine transformation: E(x) = (a * x + b) mod 26\n",
    "            y = (a * x + b) % 26\n",
    "            result += chr(y + ord(base))\n",
    "        else:\n",
    "            result += char\n",
    "    return result\n",
    "\n",
    "# -------------------------------\n",
    "# FEATURE COMPARISON FUNCTIONS\n",
    "# -------------------------------\n",
    "\n",
    "def one_gram_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two texts based on character frequencies.\n",
    "    Only considers alphanumeric characters after converting to lower case.\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Calculate 1-gram character frequencies and use cosine similarity between the two vectors\n",
    "\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def bow_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two texts based on word frequencies.\n",
    "    Splits the text on whitespace after converting to lower case.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: Calculate bag of words and use cosine similarity between the two vectors\n",
    "\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Lorem Ipsum text\n",
    "text = (\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor \"\n",
    "        \"incididunt ut labore et dolore magna aliqua.\")\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "\n",
    "# Encode using different ciphers\n",
    "caesar_encoded = caesar_cipher(text, 3)  # Caesar with shift 3\n",
    "vigenere_encoded = vigenere_cipher(text, \"key\")  # Vigenère with keyword 'key'\n",
    "\n",
    "# Substitution cipher: define a substitution mapping (e.g., a scrambled alphabet)\n",
    "alphabet = string.ascii_lowercase\n",
    "# Example mapping using a predetermined scramble. (For real use, ensure this mapping is a permutation.)\n",
    "subs_alphabet = \"qwertyuiopasdfghjklzxcvbnm\"\n",
    "mapping = {alphabet[i]: subs_alphabet[i] for i in range(26)}\n",
    "substitution_encoded = substitution_cipher(text, mapping)\n",
    "\n",
    "affine_encoded = affine_cipher(text, 5, 8)  # Affine with a=5 and b=8\n",
    "rot13_encoded = rot13(text)  # ROT13\n",
    "\n",
    "# Show the encoded texts\n",
    "print(\"\\nEncoded texts:\")\n",
    "print(\"Caesar Cipher:      \", caesar_encoded)\n",
    "print(\"Vigenère Cipher:    \", vigenere_encoded)\n",
    "print(\"Substitution Cipher:\", substitution_encoded)\n",
    "print(\"Affine Cipher:      \", affine_encoded)\n",
    "print(\"ROT13:              \", rot13_encoded)\n",
    "\n",
    "letter_string = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"\n",
    "\n",
    "print('Kidnapping letter text:', letter_string)\n",
    "\n",
    "# Save the encoded texts in a dictionary for easy iteration\n",
    "encoded_versions = {\n",
    "    \"Caesar\": caesar_encoded,\n",
    "    \"Vigenère\": vigenere_encoded,\n",
    "    \"Substitution\": substitution_encoded,\n",
    "    \"Affine\": affine_encoded,\n",
    "    \"ROT13\": rot13_encoded\n",
    "}\n",
    "\n",
    "#TODO: For each cipher, calculate the BoW and 1-gram similarity with the letter string to figure out which cipher was used\n",
    "\n",
    "print(\"\\nBag-of-Words (BoW) similarity with test string:\")\n",
    "\n",
    "\n",
    "print(\"\\nBag-of-Characters (BoC) similarity with test string:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "```\n",
    "Original text:\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "\n",
    "Encoded texts:\n",
    "Caesar Cipher:       Oruhp lsvxp groru vlw dphw, frqvhfwhwxu dglslvflqj holw, vhg gr hlxvprg whpsru lqflglgxqw xw oderuh hw groruh pdjqd doltxd.\n",
    "Vigenère Cipher:     Vspoq gzwsw hmvsp cmr kqcd, gmxwcmxcdyp khgzmqmmlq ijsx, qoh by igewkyh roqnyv gxggnmberr ex jkfmbi cd hmvspo qyqry kpgayy.\n",
    "Substitution Cipher: Sgktd ohlxd rgsgk loz qdtz, egflteztzxk qroholeofu tsoz, ltr rg toxldgr ztdhgk ofeororxfz xz sqwgkt tz rgsgkt dqufq qsojxq.\n",
    "Affine Cipher:       Lapcq wfueq xalap uwz iqcz, savucszczep ixwfwuswvm clwz, ucx xa cweuqax zcqfap wvswxwxevz ez linapc cz xalapc qimvi ilwkei.\n",
    "ROT13:               Yberz vcfhz qbybe fvg nzrg, pbafrpgrghe nqvcvfpvat ryvg, frq qb rvhfzbq grzcbe vapvqvqhag hg ynober rg qbyber zntan nyvdhn.\n",
    "Kidnapping letter text: v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\n",
    "\n",
    "Bag-of-Words (BoW) similarity with test string:\n",
    "  Caesar      : 0.0000\n",
    "  Vigenère    : 0.0000\n",
    "  Substitution: 0.0000\n",
    "  Affine      : 0.0000\n",
    "  ROT13       : 0.0000\n",
    "\n",
    "Bag-of-Characters (BoC) similarity with test string:\n",
    "  Caesar      : 0.6023\n",
    "  Vigenère    : 0.6518\n",
    "  Substitution: 0.5527\n",
    "  Affine      : 0.4254\n",
    "  ROT13       : 0.9198\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It definitely looks like the text was a ROT13 cipher! Now we could of course just directly use the ROT13 cipher to decode the text, but where's the fun in that? AI is the future right! You're using it to replace your ability to do even the most basic thinking tasks so obviously we should also use it here instead of just the closed form solution!\n",
    "\n",
    "What we need to do now is create a dataset of (ciphertext, plaintext) pairs to train a seq2seq model to decode the ciphertext. Luckily, we can use a library called NLTK to get a list of words in the English language. We can use this list to generate a dataset of (ciphertext, plaintext) pairs.\n",
    "\n",
    "[NLTK: Natural Language Toolkit](https://www.nltk.org/) is one of *the* most popular libraries for working with text in Python. It's a great tool to have in your toolbelt.\n",
    "\n",
    "In the cell below lets create our dataset of (ciphertext, plaintext) pairs, start with 100000 pairs and lets see how well the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------\n",
    "# 1. Data Preparation\n",
    "# ---------------------\n",
    "\n",
    "# Download the NLTK 'words' corpus if needed.\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Fixed vocabulary: <PAD> token plus 26 lowercase letters.\n",
    "vocab = ['<PAD>'] + ['<EOS>'] + ['<UNK>'] + ['<SOS>'] + [' '] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "vocab_size = len(vocab)  # should be 27\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for idx, ch in enumerate(vocab)}\n",
    "padding_idx = char2idx['<PAD>']\n",
    "\n",
    "# Custom Dataset: each example is a tuple (rot13_word, original_word)\n",
    "class Rot13Dataset(Dataset):\n",
    "    def __init__(self, word_list, char2idx):\n",
    "        self.data = []\n",
    "        for word in word_list:\n",
    "            # Only consider fully alphabetic words.\n",
    "            if word.isalpha():\n",
    "                word = word.lower()\n",
    "                rot13_word = codecs.encode(word, 'rot_13')\n",
    "                input_seq = torch.tensor([char2idx[c] for c in rot13_word], dtype=torch.long)\n",
    "                target_seq = torch.tensor([char2idx[c] for c in word], dtype=torch.long)\n",
    "                self.data.append((input_seq, target_seq))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Collate function to pad sequences.\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=padding_idx)\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=padding_idx)\n",
    "    return inputs_padded, targets_padded\n",
    "\n",
    "# Get words from NLTK and take a subset.\n",
    "word_list = words.words()\n",
    "filtered_words = [w for w in word_list if w.isalpha()]\n",
    "subset_words = filtered_words[:100000]\n",
    "\n",
    "dataset = Rot13Dataset(subset_words, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "print(f\"Total training samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy RNN Model\n",
    "\n",
    "Given this dataset, we now need a seq2seq model to decode the ciphertext. We'll start with a simple RNN model that you'll need to implement in numpy\n",
    "\n",
    "![Recurrent Neural Network](https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png)\n",
    "\n",
    "Remember that unlike a FFN, data in an RNN loops around over the sequence of inputs, to allow us to build up a hidden state (context in my words) that can be used to generate the next output. There's a really good [cheatsheet from Stanford](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) about RNNs that I'd recommend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "       # TODO: Implement Tanh activation function\n",
    "       pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass for Tanh activation function\n",
    "        pass\n",
    "\n",
    "    def backward(self, x):\n",
    "        # TODO: Implement backward pass for Tanh activation function\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        # TODO: Implement update function for Tanh activation function\n",
    "        pass\n",
    "\n",
    "# ---------------------\n",
    "# Fully Connected (Linear) Layer (for final projection)\n",
    "# ---------------------\n",
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # TODO: Implement initialization for LinearLayer\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: Implement forward pass for LinearLayer\n",
    "        pass\n",
    "\n",
    "    def backward(self, dA):\n",
    "        # TODO: Implement backward pass for LinearLayer\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        # TODO: Implement update function for LinearLayer\n",
    "        pass\n",
    "\n",
    "class LinearContextLayer:\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        # TODO: Implement initialization for LinearContextLayer\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        # TODO: Implement forward pass for LinearContextLayer\n",
    "        pass\n",
    "\n",
    "    def backward(self, d_outputs):\n",
    "        # TODO: Implement backward pass for LinearContextLayer\n",
    "        pass\n",
    "\n",
    "    def update(self, lr):\n",
    "        # TODO: Implement update function for LinearContextLayer\n",
    "        pass\n",
    "\n",
    "# ---------------------\n",
    "# Embedding Layer\n",
    "# ---------------------\n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        self.W = np.random.randn(vocab_size, embed_size) * 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W[x]\n",
    "\n",
    "# ---------------------\n",
    "# Complete Numpy RNN Model\n",
    "# ---------------------\n",
    "class NumpyClassRNN:\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, padding_idx=0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # Layers:\n",
    "        self.embedding = EmbeddingLayer(vocab_size, embed_size)\n",
    "\n",
    "        # For the recurrent context layer, the input dimension is the embed_size.\n",
    "        self.context = LinearContextLayer(embed_size, hidden_size)\n",
    "\n",
    "        # Final fully connected layer: project hidden state to vocabulary logits.\n",
    "        self.linear = LinearLayer(hidden_size, vocab_size)\n",
    "\n",
    "        # Keep model layers in a list for easy backward and update passes.\n",
    "        self.model = [self.context, self.linear]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: (batch_size, seq_len) with integer token indices.\n",
    "        Returns:\n",
    "          logits: (batch_size, seq_len, vocab_size)\n",
    "          outputs: (batch_size, seq_len, hidden_size) final hidden states over time.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Implement forward pass for NumpyClassRNN\n",
    "\n",
    "        return self.logits, self.outputs\n",
    "\n",
    "    def backward(self, d_logits, learning_rate=0.001):\n",
    "       # TODO: Implement backward pass for NumpyClassRNN\n",
    "       pass\n",
    "\n",
    "# ---------------------\n",
    "# Loss Function (Cross-Entropy) and Gradient\n",
    "# ---------------------\n",
    "def cross_entropy_loss_and_grad(logits, targets, padding_idx):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss and its gradient.\n",
    "    \n",
    "    Args:\n",
    "      logits: (batch_size, seq_len, vocab_size)\n",
    "      targets: (batch_size, seq_len) integer indices.\n",
    "      padding_idx: token index for padding to be ignored.\n",
    "      \n",
    "    Returns:\n",
    "      loss: scalar loss averaged over the batch.\n",
    "      d_logits: gradient of loss with respect to logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement cross-entropy loss and gradient\n",
    "\n",
    "    return loss, d_logits\n",
    "\n",
    "# ---------------------\n",
    "# 4. Training Loop (NumPy version)\n",
    "# ---------------------\n",
    "\n",
    "# Instantiate our NumPy-based RNN.\n",
    "model = NumpyClassRNN(vocab_size=vocab_size, embed_size=32, hidden_size=64, padding_idx=padding_idx)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# TODO: Implement training loop\n",
    "for epoch in range(num_epochs):\n",
    "   pass \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My training output\n",
    "\n",
    "```\n",
    "Training time: 5m 21.2s\n",
    "Epoch 1/100 - Loss: 28.0207\n",
    "Epoch 26/100 - Loss: 16.5287\n",
    "Epoch 51/100 - Loss: 6.4490\n",
    "Epoch 76/100 - Loss: 3.0717\n",
    "```\n",
    "\n",
    "This is extremely approximate output of what you should expect.\n",
    "\n",
    "### Cracking the kidnapping letter cipher\n",
    "\n",
    "Now that we have our trained RNN, we can use the kidnapping ciphertext as our input and see if we can decode it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# 5. Testing / Prediction Function\n",
    "# ---------------------\n",
    "\n",
    "def predict(model, input_str, char2idx, idx2char):\n",
    "    # TODO: Implement prediction function for each word  in the input string\n",
    "    pass\n",
    "\n",
    "    return predicted_str\n",
    "\n",
    "kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx nopqr ba ybpxre ba gur frpbaq sybbe bs svgm bgurejvfr v nz va erny gebhoyr\"\n",
    "\n",
    "predicted_original = predict(model, kidnapping_letter, char2idx, idx2char)\n",
    "print(f\"Input (ROT13): {kidnapping_letter}\")\n",
    "print(f\"Predicted original: {predicted_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to show you the expected out of the kidnapping letter as that'd spoil some of the surprise but absolutely do NOT expect to get perfectly clean english out of this, I could make out most of the words but it's pretty garbled and unfortunately because it's not perfect we can't quite get the correct padlock combination from it. However, chatGPT is of course the greatest thing since sliced bread, so maybe if we use a transformer model it will do better! It's what's already doing this homework for you, surely it can also crack the kidnapping letter cipher!\n",
    "\n",
    "### Training a Transformer model\n",
    "\n",
    "Now that we have a working RNN model, let's try training a Transformer model to see if it can do any better. For the transformer we'll just be using the pytorch implementation of one. We'll need to set up our dataset a little differently, but that's been done for you. My model trained for about an 60 minutes with approximately 270,000 pairs and I still couldn't get a perfect translation. Don't feel like you have to let yours train for that long, feel free to use a smaller dataset, but you will get significantly worse results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    "import codecs\n",
    "\n",
    "# ---------------------\n",
    "# 1. Data Preparation\n",
    "# ---------------------\n",
    "\n",
    "# Download the NLTK 'words' corpus if needed.\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Fixed vocabulary: <PAD>, <EOS>, <UNK>, <SOS>, space, and 26 lowercase letters.\n",
    "vocab = ['<PAD>', '<EOS>', '<UNK>', '<SOS>', ' '] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "vocab_size = len(vocab)  # should be 31 (if 4 specials + space + 26 letters)\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for idx, ch in enumerate(vocab)}\n",
    "padding_idx = char2idx['<PAD>']\n",
    "\n",
    "# Custom Dataset: each example is a tuple (rot13_word, original_word)\n",
    "class Rot13Dataset(Dataset):\n",
    "    def __init__(self, word_list, char2idx):\n",
    "        self.data = []\n",
    "        for word in word_list:\n",
    "            # Only consider fully alphabetic words.\n",
    "            if word.isalpha():\n",
    "                word = word.lower()\n",
    "                rot13_word = codecs.encode(word, 'rot_13')\n",
    "                # Create input sequence from the ROT13 word.\n",
    "                input_seq = torch.tensor(\n",
    "                    [char2idx.get(c, char2idx['<UNK>']) for c in rot13_word],\n",
    "                    dtype=torch.long\n",
    "                )\n",
    "                # Create target sequence from the original word.\n",
    "                target_seq = torch.tensor(\n",
    "                    [char2idx.get(c, char2idx['<UNK>']) for c in word],\n",
    "                    dtype=torch.long\n",
    "                )\n",
    "                self.data.append((input_seq, target_seq))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.data[idx]\n",
    "        # Prepare decoder input by prepending <SOS>\n",
    "        tgt_input = torch.cat([torch.tensor([char2idx['<SOS>']], dtype=torch.long), target_seq])\n",
    "        # Prepare decoder output by appending <EOS>\n",
    "        tgt_output = torch.cat([target_seq, torch.tensor([char2idx['<EOS>']], dtype=torch.long)])\n",
    "        return input_seq, tgt_input, tgt_output\n",
    "\n",
    "# Collate function to pad sequences.\n",
    "def collate_fn(batch):\n",
    "    # Each sample now is a triple: (input_seq, tgt_input, tgt_output)\n",
    "    inputs, tgt_inputs, tgt_outputs = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=padding_idx)\n",
    "    tgt_inputs_padded = pad_sequence(tgt_inputs, batch_first=True, padding_value=padding_idx)\n",
    "    tgt_outputs_padded = pad_sequence(tgt_outputs, batch_first=True, padding_value=padding_idx)\n",
    "    return inputs_padded, tgt_inputs_padded, tgt_outputs_padded\n",
    "\n",
    "# Get words from NLTK and take a subset.\n",
    "word_list = words.words()\n",
    "filtered_words = [w for w in word_list if w.isalpha()]\n",
    "subset_words = filtered_words[:1000]\n",
    "\n",
    "dataset = Rot13Dataset(subset_words, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "print(f\"Total training samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "         super(PositionalEncoding, self).__init__()\n",
    "         pe = torch.zeros(max_len, d_model)\n",
    "         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "         pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "         self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # x has shape (batch, seq_len, d_model)\n",
    "         # Ensure that the positional encodings cover the maximum possible length\n",
    "         x = x + self.pe[:, :x.size(1)]\n",
    "         return x\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build the Transformer Model\n",
    "# -----------------------------\n",
    "class Rot13Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                 num_decoder_layers, dim_feedforward, dropout, max_len):\n",
    "         super(Rot13Transformer, self).__init__()\n",
    "\n",
    "        # TODO: Implement initialization for Rot13Transformer\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "         #TODO: Implement forward pass for Rot13Transformer\n",
    "            #TODO: Embed and scale the source tokens\n",
    "            #TODO: Embed and scale the target tokens\n",
    "            #TODO: Note: Transformer expects input shape (seq_len, batch, d_model) so we transpose\n",
    "            #TODO: Transpose back and pass through final linear layer to obtain vocabulary logits\n",
    "\n",
    "         return output\n",
    "\n",
    "# Helper function to create a subsequent mask for the decoder (to prevent access to future tokens)\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Rot13Dataset(subset_words, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = 100\n",
    "\n",
    "# Initialize the transformer model with hyperparameters\n",
    "model = Rot13Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    max_len=max_len\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    #TODO: Implement training loop\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My training output\n",
    "\n",
    "```\n",
    "Training time: 63m 1.3s\n",
    "Epoch 1, Loss: 0.5194\n",
    "Epoch 2, Loss: 0.0822\n",
    "Epoch 3, Loss: 0.0467\n",
    "Epoch 4, Loss: 0.0357\n",
    "Epoch 5, Loss: 0.0279\n",
    "Epoch 6, Loss: 0.0236\n",
    "Epoch 7, Loss: 0.0197\n",
    "Epoch 8, Loss: 0.0181\n",
    "Epoch 9, Loss: 0.0161\n",
    "Epoch 10, Loss: 0.0150\n",
    "Epoch 11, Loss: 0.0136\n",
    "Epoch 12, Loss: 0.0126\n",
    "Epoch 13, Loss: 0.0115\n",
    "Epoch 14, Loss: 0.0105\n",
    "Epoch 15, Loss: 0.0105\n",
    "Epoch 16, Loss: 0.0099\n",
    "Epoch 17, Loss: 0.0092\n",
    "Epoch 18, Loss: 0.0083\n",
    "Epoch 19, Loss: 0.0088\n",
    "Epoch 20, Loss: 0.0078\n",
    "Epoch 21, Loss: 0.0070\n",
    "Epoch 22, Loss: 0.0070\n",
    "Epoch 23, Loss: 0.0068\n",
    "Epoch 24, Loss: 0.0066\n",
    "Epoch 25, Loss: 0.0065\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word_by_word(model, text, char2idx, idx2char, max_len):\n",
    "    model.eval()\n",
    "    # Split the input text into words using whitespace\n",
    "    words = text.split()\n",
    "    translated_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        pass\n",
    "        #TODO: Preprocess each word: lowercase and convert each character to its index,\n",
    "        #TODO: mapping any unknown char to <UNK> (if available)\n",
    "        #TODO: Pad or truncate to max_len\n",
    "        \n",
    "        #TODO: Decoder starts with the <SOS> token\n",
    "        \n",
    "        #TODO: Autoregressive decoding loop: generate one token at a time for this word\n",
    "        \n",
    "        #TODO: Convert predicted indices into characters and filter out special tokens\n",
    "    \n",
    "    #TODO: Join the translated words into a complete sentence\n",
    "    return None\n",
    "\n",
    "kidnapping_letter = \"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\"\n",
    "\n",
    "translated = translate_word_by_word(model, kidnapping_letter, char2idx, idx2char, max_len)\n",
    "print(\"ROT13 Input:\", kidnapping_letter)\n",
    "print(\"Translated (English):\", translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "I trained the model for about an hour with 270,000 pairs and still was unable to get a perfect translation. There are tricks you can play with the model at inference time to do it, but I don't expect you to do that.\n",
    "\n",
    "### AI Falling Short\n",
    "\n",
    "DAMN! After all of this, AI still can't crack the kidnapping letter cipher! Defeated by a simple rot13 cipher, the dumbest of all ciphers! Accepting defeat, you still need to solve the murder! Unfortunately, all we have to do is run the ciphertext back through the rot13 function to get the original message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot13_text = rot13(\"v ybirq zheqrevat ze gurvfrasyblq ng gur rfgngr gur tnf jnf gur cresrpg zheqre jrncba vz fher vyy trg njnl jvgu vg nf jryy ubcrshyyl abobql svtherf bhg gur pbzovangvba bs gur cnqybpx cynl ba ybpxre 69 ba gur frpbaq sybbe bs phfuvat bgurejvfr v nz va erny gebhoyr\")\n",
    "\n",
    "print(\"ROT13 Output:\", rot13_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe sometimes we just shouldn't use AI to solve problems. Something to think about.\n",
    "\n",
    "### Final Task\n",
    "\n",
    "Now that you have the code for the locker, hopefully you have enough to put this villain behind bars! Head over to the locker and open it, take a picture of the evidence and yourselves and submit it along with your code, the TAs will handle the police report! Remember, this is an active investigation and tampering with the evidence will result in a felony charge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
