<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>be9724d79a014638bceb4d01691bef8e</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="homework-04-optical-character-recognition"
class="cell markdown" id="S_HwBNlcqzrs">
<h3>Homework 04: Optical Character Recognition</h3>
<p>Now that you have the segmented letters from the previous task, we
need a way to actually convert the letters to text! You can't be
bothered to just transcribe the images yourself, but you remember your
professor droning on about something called MNIST and you think that
these letters might be kind of similar to handwritten digits.</p>
<p>Unfortunately, because your professor hates you, he's making you
write a FFN using only numpy for the first part of this assignment. Use
the dataset available from the following link for training, testing, and
validation on this assignment. <a
href="https://drive.google.com/drive/folders/1xK3Mp9BhXWpae-ZicfGtTqkVRW-x8ntI?usp=sharing">Alphabet
Cuttings Dataset</a></p>
<p>The code immediately below is for loading and formatting the dataset.
You don't have to do anything here yourself.</p>
</section>
<div class="cell code" id="Q-HeldcPqzrt">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_rgb_contours(input_path, display<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Detect contours in the RGB channels of a PNG image and draw all contours in hierarchy.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        input_path (str): Path to the input PNG image</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        line_thickness (int): Thickness of contour lines in pixels</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Read the image with alpha channel</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> cv2.imread(input_path, cv2.IMREAD_UNCHANGED)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the RGB channels</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    rgb_img <span class="op">=</span> img[:, :, :<span class="dv">3</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to grayscale for contour detection</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    gray <span class="op">=</span> cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> display:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        display(Image.fromarray(gray))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Setting parameter values</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    t_lower <span class="op">=</span> <span class="dv">50</span>  <span class="co"># Lower Threshold</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    t_upper <span class="op">=</span> <span class="dv">150</span>  <span class="co"># Upper threshold</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Applying the Canny Edge filter</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    edge <span class="op">=</span> cv2.Canny(gray, t_lower, t_upper)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Close the edges to form complete contours</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> display:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        display(Image.fromarray(edge))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find contours recursively</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    contours, hierarchy <span class="op">=</span> cv2.findContours(edge, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a separate image for each contour with different colors</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    mnist_img <span class="op">=</span> np.zeros((rgb_img.shape[<span class="dv">0</span>], rgb_img.shape[<span class="dv">1</span>]), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> display:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(mnist_img.shape)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="bu">len</span>(hierarchy))</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(hierarchy)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a different color for each contour based on index</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, contour <span class="kw">in</span> <span class="bu">enumerate</span>(contours):</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            cv2.drawContours(mnist_img, [contour], <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, thickness<span class="op">=</span>cv2.FILLED)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            cv2.drawContours(mnist_img, [contour], <span class="op">-</span><span class="dv">1</span>, <span class="dv">255</span>, thickness<span class="op">=</span>cv2.FILLED)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    mnist_img <span class="op">=</span> cv2.resize(mnist_img, (<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the result with multiple contours</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> display:</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        plt.imshow(mnist_img, cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="ss">f&quot;All </span><span class="sc">{</span><span class="bu">len</span>(contours)<span class="sc">}</span><span class="ss"> contours with unique colors&quot;</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mnist_img</span></code></pre></div>
</div>
<div class="cell code" id="Cnf5vqf8qzrt">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, Tuple</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_letter_dataset(data_dir: <span class="bu">str</span>, train_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">7</span>, test_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>, holdout_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>) <span class="op">-&gt;</span> Dict:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Load and split letter dataset into train, test, and holdout sets.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Verify split sizes</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> train_size <span class="op">+</span> test_size <span class="op">+</span> holdout_size <span class="op">==</span> <span class="dv">10</span>, <span class="st">&quot;Split sizes must sum to 10&quot;</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dictionary to store all instances of each letter</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    letter_instances <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Collect all image paths</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> filename <span class="kw">in</span> os.listdir(data_dir):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> filename.endswith(<span class="st">&#39;.png&#39;</span>) <span class="kw">and</span> <span class="kw">not</span> filename[<span class="dv">0</span>].isdigit():</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            letter <span class="op">=</span> filename[<span class="dv">0</span>]  <span class="co"># First character is the letter</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            instance_path <span class="op">=</span> os.path.join(data_dir, filename)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            letter_instances[letter].append(instance_path)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    train_data <span class="op">=</span> {<span class="st">&#39;images&#39;</span>: [], <span class="st">&#39;labels&#39;</span>: []}</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    test_data <span class="op">=</span> {<span class="st">&#39;images&#39;</span>: [], <span class="st">&#39;labels&#39;</span>: []}</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    holdout_data <span class="op">=</span> {<span class="st">&#39;images&#39;</span>: [], <span class="st">&#39;labels&#39;</span>: []}</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each letter</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> letter, instances <span class="kw">in</span> letter_instances.items():</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Randomly shuffle the instances</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        random.shuffle(instances)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into train/test/holdout</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        train_paths <span class="op">=</span> instances[:train_size]</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        test_paths <span class="op">=</span> instances[train_size:train_size <span class="op">+</span> test_size]</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        holdout_paths <span class="op">=</span> instances[train_size <span class="op">+</span> test_size:]</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load images and add to respective sets</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> path <span class="kw">in</span> train_paths:</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> detect_rgb_contours(path)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>            train_data[<span class="st">&#39;images&#39;</span>].append(img)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>            train_data[<span class="st">&#39;labels&#39;</span>].append(letter)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> path <span class="kw">in</span> test_paths:</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> detect_rgb_contours(path)</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>            test_data[<span class="st">&#39;images&#39;</span>].append(img)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>            test_data[<span class="st">&#39;labels&#39;</span>].append(letter)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> path <span class="kw">in</span> holdout_paths:</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> detect_rgb_contours(path)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>            holdout_data[<span class="st">&#39;images&#39;</span>].append(img)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>            holdout_data[<span class="st">&#39;labels&#39;</span>].append(letter)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(train_data[<span class="st">&#39;labels&#39;</span>][<span class="dv">0</span>], train_data[<span class="st">&#39;images&#39;</span>][<span class="dv">0</span>].shape)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    plt.imshow(train_data[<span class="st">&#39;images&#39;</span>][<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to numpy arrays</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> dataset <span class="kw">in</span> [train_data, test_data, holdout_data]:</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        dataset[<span class="st">&#39;images&#39;</span>] <span class="op">=</span> np.array(dataset[<span class="st">&#39;images&#39;</span>])</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        dataset[<span class="st">&#39;labels&#39;</span>] <span class="op">=</span> np.array(dataset[<span class="st">&#39;labels&#39;</span>])</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;train&#39;</span>: train_data,</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;test&#39;</span>: test_data,</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;holdout&#39;</span>: holdout_data</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_data(data_dict: Dict) <span class="op">-&gt;</span> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a><span class="co">    Prepare data for FFN training:</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a><span class="co">    - Preprocess all images</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="co">    - Convert labels to numerical format</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="co">    - Split into features (X) and labels (y)</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process training data</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> np.array([img.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> <span class="dv">255</span> <span class="cf">for</span> img <span class="kw">in</span> data_dict[<span class="st">&#39;train&#39;</span>][<span class="st">&#39;images&#39;</span>]])</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> np.array([img.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> <span class="dv">255</span> <span class="cf">for</span> img <span class="kw">in</span> data_dict[<span class="st">&#39;test&#39;</span>][<span class="st">&#39;images&#39;</span>]])</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert labels to numerical format</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>    label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> label_encoder.fit_transform(data_dict[<span class="st">&#39;train&#39;</span>][<span class="st">&#39;labels&#39;</span>])</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> label_encoder.transform(data_dict[<span class="st">&#39;test&#39;</span>][<span class="st">&#39;labels&#39;</span>])</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save label encoder mapping for reference</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>    label_mapping <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Label mapping:&quot;</span>, label_mapping)</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X_train, X_test, y_train, y_test</span></code></pre></div>
</div>
<div class="cell markdown" id="GeyqaOozqzru">
<p><strong>Neural Network from Scratch</strong></p>
<p>Your task is to implement a simple neural network from scratch in
numpy to classify the letters in the dataset following the architecture
shown below.</p>
<p>In order to actually implement a training regime for our network,
we'll need to specify a loss function that we can use to measure how
well our network is doing. We'll use the cross entropy loss function as
we're attempting a multiclass classification task.</p>
<p><img src="https://pbs.twimg.com/media/FBmVmdHWQAAU7gq.png"
alt="Cross Entropy Loss" /></p>
<p>Training our network will consist of two steps primarily, forward
propagation and back propagation.</p>
<p>Forward propagation is the process of taking our input data, and
passing it through the network to get a prediction.</p>
<p>Back propagation is the process of taking the derivative of the loss
function with respect to the weights and biases, and using gradient
descent to update the weights and biases.</p>
<p><img
src="https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/blueprint.gif"
alt="NN Training" /></p>
<p>In this gif we can see a brief outline of the forward and backward
propagation steps.</p>
<p>Broadly speaking, forward is what gives us our prediction, and
backward is what gives us the gradient of the loss function with respect
to the weights and biases, and is how we update the weights to get
closer to the right answer (done by minimizing the loss function).</p>
</div>
<div class="cell markdown" id="_CuYXmZZqzru">
<p>We'll also need to implement a couple activation functions and their
derivatives.</p>
<p>We're going to be using the ReLU activation function for our hidden
layers, and a softmax function for our output layer. The softmax will
allow us to map our output to a probability between 0 and 1 and from
there to a class based on an argmax operation.</p>
<p><img
src="https://raw.githubusercontent.com/SkalskiP/ILearnDeepLearning.py/e300c61fc39e480bad8d4d83616e763334b74ec7/01_mysteries_of_neural_networks/03_numpy_neural_net/supporting_visualizations/activations.gif"
alt="Activation Functions" /></p>
<p>Here we can see both activation functions and their derivatives.</p>
<p>The part that most people find tricky about this is the
backpropagation step.</p>
<p>As we've seen in class for "single layer" examples, to optimize the
weights of a model using gradient descent, we can rewrite the loss
function in terms of the weights and then take partial derivatives with
respect to each weight.</p>
<p><img
src="https://global.discourse-cdn.com/dlai/original/3X/f/5/f58df86a4c92695569d9536d7e752161cd0f98fb.jpeg"
alt="Gradient Descent" /></p>
<p>Will multilayer networks, how do we take the derivative of the loss
function with respect to the weights, if the weights in the previous
layer are reliant on the weights in the layer before them?</p>
<p>Backpropagation is the solution to this and revolves around using the
chain rule to take essentially a series of partial derivatives backwards
through the network to get the gradient of the loss function with
respect to the weights at each layer. We can then redistribute these
gradients to update the weights of the network.</p>
<p><img
src="https://miro.medium.com/v2/resize:fit:1200/0*9lo2ux8ASvt6YJkH.gif"
alt="Backprop" /></p>
</div>
<div class="cell markdown" id="1CX7m-iAqzru">
<p><strong>BETTER TEACHING</strong></p>
<p>To be honest, your best bet is to watch the youtube videos by
3Blue1Brown. He's an incredible teacher and will do a better job than I
can, along with better visualizations.</p>
<p><a href="https://www.youtube.com/watch?v=aircAruvnKk"><img
src="https://img.youtube.com/vi/aircAruvnKk/0.jpg"
alt="Introduction to Neural Networks" /></a></p>
<p>This is an introduction to neural networks using the MNIST
dataset!</p>
<p>Then we have a great video on gradient descent.</p>
<p><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w"><img
src="https://img.youtube.com/vi/IHZwWFHWa-w/0.jpg"
alt="Gradient Descent" /></a></p>
<p>Finally I'd recommend at least his first video on backpropagation,
though you should probably watch the second too.</p>
<p><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U"><img
src="https://img.youtube.com/vi/Ilg3gGewQ5U/0.jpg"
alt="Backprop" /></a></p>
</div>
<div class="cell code" id="v_sCyMDgqzru">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NumpyNeuralNetwork:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Here we define the number and types of layers in our network</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we also include their activation functions</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: You&#39;ll almost certainly need to add some more layers to get to 70% accuracy</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    NN_ARCHITECTURE <span class="op">=</span> [</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;input_dim&quot;</span>: <span class="dv">784</span>, <span class="st">&quot;output_dim&quot;</span>: <span class="dv">37</span>, <span class="st">&quot;activation&quot;</span>: <span class="st">&quot;relu&quot;</span>},</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        {<span class="st">&quot;input_dim&quot;</span>: <span class="dv">37</span>, <span class="st">&quot;output_dim&quot;</span>: <span class="dv">26</span>, <span class="st">&quot;activation&quot;</span>: <span class="st">&quot;softmax&quot;</span>},</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Our init function just initializes the weights and biases for each layer</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, seed <span class="op">=</span> <span class="dv">42</span>):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># random seed initiation</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        np.random.seed(seed)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># parameters storage initiation</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params_values <span class="op">=</span> {}</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iteration over network layers</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.NN_ARCHITECTURE):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># we number network layers from 1</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            layer_idx <span class="op">=</span> idx <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># extracting the number of units in layers</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            layer_input_size <span class="op">=</span> layer[<span class="st">&quot;input_dim&quot;</span>]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            layer_output_size <span class="op">=</span> layer[<span class="st">&quot;output_dim&quot;</span>]</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            <span class="co"># initiating the values of the W matrix</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># and vector b for subsequent layers</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.params_values[<span class="st">&#39;W&#39;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)] <span class="op">=</span> np.random.randn(</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>                layer_output_size, layer_input_size) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.params_values[<span class="st">&#39;b&#39;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)] <span class="op">=</span> np.random.randn(</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>                layer_output_size, <span class="dv">1</span>) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_params_values(<span class="va">self</span>):</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.params_values</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the relu function</span></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> relu(<span class="va">self</span>, Z):</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a><span class="co">        Applies the ReLU (Rectified Linear Unit) activation function.</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a><span class="co">           - Z: NumPy array of pre-activation values from a layer</span></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="co">           - A: NumPy array with ReLU outputs</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: Why is Z in a perceptron a number but Z in a neural network is a matrix of numbers?</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What are the dimensions of the Z vector? Don&#39;t answer with a specific number but a generalizable statement</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the relu_backward function</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> relu_backward(<span class="va">self</span>, dA, Z):</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co">        Perform the backward pass for the ReLU activation function.</span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co">          - dA: Gradient of the loss with respect to the activation output (A) from the current layer</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co">          - Z:  The input to the activation function of currently layer</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="co">           - dZ: Gradient of the loss with respect to the input (Z) of the current layers ReLU activation function</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What is the purpose of setting the gradient dZ to 0 for elements where Z≤0 in the ReLU backward function?</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What is the calculated dZ (the returned matrix) of this function used for?</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the softmax function</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, Z):</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the softmax activation function for the given input Z.</span></span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="co">          - Z : Input matrix to the softmax function</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a><span class="co">          - A probability distribution representing the likelihood of each class</span></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What does the softmax function do and where is it normally used in a neural network?</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the softmax_backward function</span></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax_backward(<span class="va">self</span>, dA, Z):</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the gradient of the loss with respect to Z for a softmax activation function.</span></span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a><span class="co">          - dA: Gradient of the loss with respect to the output of the softmax layer</span></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="co">          - Z: Input to the softmax function before activation</span></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a><span class="co">          - Gradient of the loss with respect to Z</span></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Hint: for cross entropy loss function, softmax_backwards becames very simple (1 line)</span></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Finish the single_layer_forward_propagation function</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> single_layer_forward_propagation(<span class="va">self</span>, A_prev, W_curr, b_curr, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>):</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs forward propagation for a single layer.</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="co">        Parameters:</span></span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="co">          - A_prev: Activation from the previous layer</span></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="co">          - W_curr: Weights for the current layer</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="co">          - b_curr: Biases for the current layer</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a><span class="co">          - activation: Activation function to apply</span></span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a><span class="co">          - A: Activation output of the current layer</span></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a><span class="co">          - Z_curr: linear transformation result before activation</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: Why do we return both A and Z_curr?</span></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: calculation of the input value for the activation function</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    hint: this looks super similar to the perceptron equation!</span></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        Z_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># selection of activation function</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> activation <span class="op">==</span> <span class="st">&quot;relu&quot;</span>:</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>            activation_func <span class="op">=</span> <span class="va">self</span>.relu</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> activation <span class="op">==</span> <span class="st">&quot;sigmoid&quot;</span>:</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a>            activation_func <span class="op">=</span> <span class="va">self</span>.sigmoid</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">&#39;Non-supported activation function&#39;</span>)</span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: return of calculated activation A and the intermediate Z matrix</span></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Finish the full_forward_propagation function</span></span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> full_forward_propagation(<span class="va">self</span>, X):</span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs forward propagation through the entire neural network.</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a><span class="co">           - X : input data</span></span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a><span class="co">          - A_curr : final activation output of the network</span></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a><span class="co">          - memory : dictionary storing intermediate A and Z values for backpropagation</span></span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a>        <span class="co"># creating a temporary memory to store the information needed for a backward step</span></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a>        memory <span class="op">=</span> {}</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X vector is the activation for layer 0</span></span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a>        A_curr <span class="op">=</span> X</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iteration over network layers</span></span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.NN_ARCHITECTURE):</span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># we number network layers from 1</span></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>            layer_idx <span class="op">=</span> idx <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a>            <span class="co"># transfer the activation from the previous iteration</span></span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>            A_prev <span class="op">=</span> A_curr</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: extraction of the activation function for the current layer</span></span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a>            activ_function_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: extraction of W for the current layer</span></span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a>            W_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: extraction of b for the current layer</span></span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a>            b_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: calculation of activation for the current layer</span></span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a>            A_curr, Z_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a>            <span class="co"># saving calculated values in the memory</span></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>            memory[<span class="st">&quot;A&quot;</span> <span class="op">+</span> <span class="bu">str</span>(idx)] <span class="op">=</span> A_prev</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a>            memory[<span class="st">&quot;Z&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)] <span class="op">=</span> Z_curr</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return of prediction vector and a dictionary containing intermediate values</span></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> A_curr, memory</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_cost_value(<span class="va">self</span>, Y_hat, Y):</span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the cost of the neural network&#39;s predictions</span></span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y_hat: The predicted probabilities</span></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y:     ground truth labels</span></span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a><span class="co">          - The computed loss</span></span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What are the dimensions of Y_hat and Y?</span></span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of examples</span></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> Y_hat.shape[<span class="dv">1</span>]</span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculation of the cost according to the formula</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">/</span> m <span class="op">*</span> (np.dot(Y, np.log(Y_hat).T) <span class="op">+</span> np.dot(<span class="dv">1</span> <span class="op">-</span> Y, np.log(<span class="dv">1</span> <span class="op">-</span> Y_hat).T))</span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.squeeze(cost)</span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the convert_prob_into_class function</span></span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> convert_prob_into_class(<span class="va">self</span>, probs):</span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a><span class="co">        Converts probability values from the softmax function into discrete class predictions</span></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a><span class="co">          - probs : 2D array where each col represents the predicted probability distribution</span></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a><span class="co">                    over classes for a given example</span></span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a><span class="co">          - probs_ : 1D array where each value represents the predicted class for each example</span></span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a>        probs_ <span class="op">=</span> np.copy(probs)</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> probs_.flatten()</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_accuracy_value(<span class="va">self</span>, Y_hat, Y):</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a><span class="co">        Computes the accuracy of the model&#39;s predictions by comparing them with the true labels</span></span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y_hat:</span></span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y:</span></span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a><span class="co">          - The accuracy of the model’s predictions, which is the fraction of correctly predicted labels</span></span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a>        Y_hat_ <span class="op">=</span> <span class="va">self</span>.convert_prob_into_class(Y_hat)</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (Y_hat_ <span class="op">==</span> Y).<span class="bu">all</span>(axis<span class="op">=</span><span class="dv">0</span>).mean()</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Write the single_layer_backward_propagation function</span></span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> single_layer_backward_propagation(<span class="va">self</span>, dA_curr, W_curr, b_curr, Z_curr, A_prev, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>):</span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs backward propagation for a single layer to calculate the gradients of the cost function</span></span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a><span class="co">        with respect to the weights, biases, and activations</span></span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a><span class="co">           - dA_curr: The gradient of the loss with respect to the activation output (A) from the current layer</span></span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a><span class="co">           - W_curr : The weights for the current layer</span></span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a><span class="co">           - b_curr : The biases for the current layer</span></span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a><span class="co">           - Z_curr : The linear transformation result (Z) before activation for the current layer</span></span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a><span class="co">           - A_prev : The activation from the previous layer</span></span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a><span class="co">           - activation : The activation function used in the current layer (&quot;relu&quot; or &quot;sigmoid&quot;)</span></span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a><span class="co">           - dA_prev : The gradient of the loss with respect to the activation of the previous layer (used for backpropagation)</span></span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a><span class="co">           - dW_curr : The gradient of the cost function with respect to the weights (used for weight updates)</span></span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a><span class="co">           - db_curr : The gradient of the cost function with respect to the biases  (used for bias updates)</span></span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: What is the significance of calculating dW_curr in backpropagation?</span></span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: After calculating the gradients for weights (dW_curr) and biases (db_curr), what is the purpose</span></span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a><span class="co">                       of the calculation dA_prev used for in backpropagation?</span></span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of examples</span></span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> A_prev.shape[<span class="dv">1</span>]</span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a>        <span class="co"># selection of activation function</span></span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> activation <span class="op">==</span> <span class="st">&quot;relu&quot;</span>:</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a>            backward_activation_func <span class="op">=</span> <span class="va">self</span>.relu_backward</span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> activation <span class="op">==</span> <span class="st">&quot;sigmoid&quot;</span>:</span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a>            backward_activation_func <span class="op">=</span> <span class="va">self</span>.sigmoid_backward</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="st">&#39;Non-supported activation function&#39;</span>)</span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: calculation of the activation function derivative</span></span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>        dZ_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: derivative of the matrix W</span></span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>        dW_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: derivative of the vector b</span></span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>        db_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: derivative of the matrix A_prev</span></span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>        dA_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dA_prev, dW_curr, db_curr</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Finish the full_backward_propagation function</span></span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> full_backward_propagation(<span class="va">self</span>, Y_hat, Y, memory):</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a><span class="co">        Performs the backward propagation through the entire neural network.</span></span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y_hat: the predicted values (activations)</span></span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y:     ground truth labels (one-hot encoded)</span></span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a><span class="co">          - memory: dictionary containing the activations and pre-activations (Z values)</span></span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a><span class="co">                for each layer during the forward pass.</span></span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a><span class="co">          - grads_values: dictionary containing the gradients of the cost function</span></span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a><span class="co">                          with respect to the weights, biases, and activations for each layer.</span></span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a><span class="co">        Concept Check: Why store the calculated gradients in a dictionary? How will they be used?</span></span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a>        grads_values <span class="op">=</span> {}</span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a>        <span class="co"># number of examples</span></span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> Y.shape[<span class="dv">1</span>]</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a hack ensuring the same shape of the prediction vector and labels vector</span></span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> Y.reshape(Y_hat.shape)</span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: initiation of gradient descent algorithm</span></span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a>           <span class="co"># hint: The initial gradient of the loss with respect to the activation can be set up using only the the predicted labels, true lables, and one mathmatical operator</span></span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>        dA_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iteration over network layers</span></span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer_idx_prev, layer <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">list</span>(<span class="bu">enumerate</span>(<span class="va">self</span>.NN_ARCHITECTURE))):</span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>            <span class="co"># we number network layers from 1</span></span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a>            layer_idx_curr <span class="op">=</span> layer_idx_prev <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a>            <span class="co"># extraction of the activation function for the current layer</span></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a>            activ_function_curr <span class="op">=</span> layer[<span class="st">&quot;activation&quot;</span>]</span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a>            dA_curr <span class="op">=</span> dA_prev</span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We get the activation from the previous layer and the Z matrix from the current layer</span></span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a>            A_prev <span class="op">=</span> memory[<span class="st">&quot;A&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_prev)]</span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a>            Z_curr <span class="op">=</span> memory[<span class="st">&quot;Z&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_curr)]</span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We get the weights and biases for the current layer</span></span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a>            W_curr <span class="op">=</span> <span class="va">self</span>.params_values[<span class="st">&quot;W&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_curr)]</span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>            b_curr <span class="op">=</span> <span class="va">self</span>.params_values[<span class="st">&quot;b&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_curr)]</span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: calculate the gradients of the cost function with respect to the weights and biases</span></span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>            dA_prev, dW_curr, db_curr <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a>            <span class="co"># We save the gradients of the cost function with respect to the weights and biases</span></span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a>            grads_values[<span class="st">&quot;dW&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_curr)] <span class="op">=</span> dW_curr</span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a>            grads_values[<span class="st">&quot;db&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx_curr)] <span class="op">=</span> db_curr</span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> grads_values</span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, grads_values):</span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a><span class="co">        Updates the weights and biases of the neural network during gradient descent.</span></span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a><span class="co">          - grads_values: dictionary containing the previously calculated gradients</span></span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a><span class="co">          - params_values: dictionary containing the updated values of the weights and biases</span></span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>        <span class="co"># iteration over network layers</span></span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer_idx, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.NN_ARCHITECTURE, <span class="dv">1</span>):</span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.params_values[<span class="st">&quot;W&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)] <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> grads_values[<span class="st">&quot;dW&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)]</span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.params_values[<span class="st">&quot;b&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)] <span class="op">-=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> grads_values[<span class="st">&quot;db&quot;</span> <span class="op">+</span> <span class="bu">str</span>(layer_idx)]</span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.params_values</span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Finish the train function</span></span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, Y, epochs<span class="op">=</span><span class="dv">100</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, batch_size<span class="op">=</span><span class="dv">8</span>, verbose<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a><span class="co">        Train the neural network using mini-batch gradient descent</span></span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a><span class="co">          - X: Input data (features), shape (n_features, n_examples)</span></span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a><span class="co">          - Y: True labels, shape (n_classes, n_examples)</span></span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a><span class="co">          - epochs: Number of training iterations</span></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a><span class="co">          - learning_rate: Learning rate for gradient descent</span></span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a><span class="co">          - batch_size: Size of each mini-batch</span></span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a><span class="co">          - verbose: If True, prints cost and accuracy at intervals</span></span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a><span class="co">        Outputs:</span></span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a><span class="co">          - Dictionary containing cost and accuracy history over epochs</span></span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initiation of lists storing the history of metrics calculated during the learning process</span></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a>        cost_history <span class="op">=</span> []</span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>        accuracy_history <span class="op">=</span> []</span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: implement mini-batch training</span></span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Mini-batch processing</span></span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a>            permutation <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a>            X_shuffled <span class="op">=</span> X[:, permutation]</span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a>            Y_shuffled <span class="op">=</span> Y[:, permutation]</span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, m, batch_size):</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>                <span class="co">#TODO: Forward propagation</span></span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a>                <span class="cf">pass</span></span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>                <span class="co">#TODO: Backward propagation</span></span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>                <span class="cf">pass</span></span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a>                <span class="co">#TODO: Update parameters</span></span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.update(grads, learning_rate)</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: Calculate metrics for the whole epoch (cost and accuracy)</span></span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Append metrics to storage</span></span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>            cost_history.append(cost)</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a>            accuracy_history.append(accuracy)</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> verbose <span class="kw">and</span> i <span class="op">%</span> <span class="dv">500</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Epoch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epochs<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Cost: </span><span class="sc">{</span>cost<span class="sc">:.5f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f&quot;Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.5f}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">&quot;-&quot;</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">&#39;cost_history&#39;</span>: cost_history, <span class="st">&#39;accuracy_history&#39;</span>: accuracy_history}</span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a><span class="co"># Comment to prevent docstrings from being printed</span></span></code></pre></div>
</div>
<section id="ffn-evaluation" class="cell markdown" id="zRdFPk8aqzrv">
<h3>FFN Evaluation</h3>
<p>The cell below will allow you to evaluate the performance of your FFN
on the holdout set.</p>
</section>
<div class="cell code" id="42Z7gBmCqzrw">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_on_holdout(data_dict, model):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Evaluate the trained model on the holdout set</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">        data_dict: Dictionary containing the dataset splits</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">        model: Trained NumpyNeuralNetwork model</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">        float: Accuracy on holdout set</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">        np.ndarray: Confusion matrix</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Preprocess holdout data</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    X_holdout <span class="op">=</span> np.array([img.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> <span class="dv">255</span> <span class="cf">for</span> img <span class="kw">in</span> data_dict[<span class="st">&#39;holdout&#39;</span>][<span class="st">&#39;images&#39;</span>]])</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get labels and convert to numerical format using the same encoder</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    label_encoder.fit(data_dict[<span class="st">&#39;train&#39;</span>][<span class="st">&#39;labels&#39;</span>])  <span class="co"># Fit on training data to maintain same mapping</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    y_holdout <span class="op">=</span> label_encoder.transform(data_dict[<span class="st">&#39;holdout&#39;</span>][<span class="st">&#39;labels&#39;</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to format needed by model</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    X_holdout <span class="op">=</span> X_holdout.T</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    y_holdout_onehot <span class="op">=</span> np.eye(<span class="dv">26</span>)[y_holdout].T</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    y_pred, _ <span class="op">=</span> model.full_forward_propagation(X_holdout)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> model.get_accuracy_value(y_pred, y_holdout_onehot)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predicted classes</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    predicted_classes <span class="op">=</span> np.argmax(y_pred, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create confusion matrix</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    conf_matrix <span class="op">=</span> confusion_matrix(y_holdout, predicted_classes)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print detailed results</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Holdout Set Evaluation:&quot;</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy, conf_matrix</span></code></pre></div>
</div>
<section id="running-our-fnn" class="cell markdown" id="9J0tN8V2qzrw">
<h3>Running our FNN</h3>
<p>Lets use all of our data to train and evaluate our FFN!</p>
</section>
<div class="cell code" id="pb0NX_Y6qzrw">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_letter_dataset(<span class="st">&quot;homework04/alphabet&quot;</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> prepare_data(data)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to proper format</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train.T</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test.T</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>y_train_onehot <span class="op">=</span> np.eye(<span class="dv">26</span>)[y_train].T</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>y_test_onehot <span class="op">=</span> np.eye(<span class="dv">26</span>)[y_test].T</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize and train model</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NumpyNeuralNetwork()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.train(X_train, y_train_onehot, batch_size<span class="op">=</span><span class="dv">32</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on holdout set</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>holdout_accuracy, conf_matrix <span class="op">=</span> evaluate_on_holdout(data, model)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize results</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training history</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">&#39;cost_history&#39;</span>])</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training Loss&#39;</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.plot(history[<span class="st">&#39;accuracy_history&#39;</span>])</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training Accuracy&#39;</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Accuracy&#39;</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot confusion matrix</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>sns.heatmap(conf_matrix, annot<span class="op">=</span><span class="va">True</span>, fmt<span class="op">=</span><span class="st">&#39;d&#39;</span>, cmap<span class="op">=</span><span class="st">&#39;Blues&#39;</span>)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Confusion Matrix on Holdout Set&#39;</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;True Label&#39;</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Predicted Label&#39;</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<section id="target-accuracy-70-on-holdout-set" class="cell markdown"
id="kEs1r9Elqzrw">
<h3>Target Accuracy: 70% on Holdout Set</h3>
<p>Instead of giving hard values, which is basically impossible in deep
learning, I'll be giving you a target output accuracy instead. Your goal
is to reach 70% accuracy on the holdout set. You'll almost certainly
have to test a number of different combinations of architectures and
hyperparameters.</p>
</section>
<section id="cnn-experiment" class="cell markdown" id="F4bMTRN5qzrw">
<h3>CNN Experiment</h3>
<p>While the FFN is okay, it's really not that well suited to image
classification tasks such as this. Fighting through the hangover, you
recall something about the news channel CNN? Implement a CNN (using
pytorch) below and see if you can get a better result than the FFN.</p>
</section>
<div class="cell code" id="UHOavdLHqzrw">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cell 1: Imports for both experiments</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LetterDataset(Dataset):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, images, labels, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.images <span class="op">=</span> images</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use LabelEncoder to encode the labels</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.label_encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels <span class="op">=</span> <span class="va">self</span>.label_encoder.fit_transform(labels)  <span class="co"># Fit and transform labels</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.images)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get image and label</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.fromarray(<span class="va">self</span>.images[idx], mode<span class="op">=</span><span class="st">&#39;L&#39;</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> <span class="va">self</span>.labels[idx]</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply transform to image if specified</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co">#TODO: Define the neural network architecture</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicCNN(nn.Module):</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">26</span>):  <span class="co"># Assuming 26 classes (A-Z)):</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Basic CNN for letter classification.</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="co">          - num_classes: Number of output classes (default: 26 for A-Z).</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="co">          - Logits (before softmax) representing class predictions.</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>      <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass of the CNN.</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs:</span></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="co">          - x: Input image tensor of shape (batch_size, 1, 28, 28).</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Output:</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="co">          - Logits for classification.</span></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="co">      &quot;&quot;&quot;</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a><span class="co">#TODO: Training function for the CNN</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, train_loader, val_loader, device, num_epochs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a><span class="co">    Trains a CNN model using mini-batch gradient descent and evaluates it on a validation set.</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a><span class="co">      - model: The neural network model to be trained</span></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="co">      - train_loader: DataLoader for the training dataset</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a><span class="co">      - val_loader: DataLoader for the validation dataset</span></span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="co">      - device: The device (CPU or GPU) to run training on</span></span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a><span class="co">      - num_epochs: Number of epochs for training</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a><span class="co">      - learning_rate: learning rate</span></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a><span class="co">      - Dictionary containing training loss, training accuracy, and validation accuracy history</span></span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a><span class="co">  &quot;&quot;&quot;</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="op">=</span> []</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>    train_accs <span class="op">=</span> []</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>    val_accs <span class="op">=</span> []</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>        total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> images, labels <span class="kw">in</span> train_loader:</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>            images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>            <span class="co"># </span><span class="al">TODO</span><span class="co">: Train the model</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> outputs.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> predicted.eq(labels).<span class="bu">sum</span>().item()</span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> running_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> <span class="fl">100.</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Validate model on validation set</span></span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">=</span> <span class="fl">100.</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a>        train_losses.append(epoch_loss)</span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>        train_accs.append(train_acc)</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a>        val_accs.append(val_acc)</span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;Epoch [</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">], Loss: </span><span class="sc">{</span>epoch_loss<span class="sc">:.4f}</span><span class="ss">, &#39;</span></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f&#39;Train Acc: </span><span class="sc">{</span>train_acc<span class="sc">:.2f}</span><span class="ss">%, Val Acc: </span><span class="sc">{</span>val_acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code" id="JQkxyr9zqzrw">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data (using your existing load_letter_dataset function)</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>data_dict <span class="op">=</span> load_letter_dataset(<span class="st">&quot;homework_datasets/alphabet&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create datasets</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> LetterDataset(data_dict[<span class="st">&#39;train&#39;</span>][<span class="st">&#39;images&#39;</span>],</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                                data_dict[<span class="st">&#39;train&#39;</span>][<span class="st">&#39;labels&#39;</span>],</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                                transform<span class="op">=</span>transform)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> LetterDataset(data_dict[<span class="st">&#39;test&#39;</span>][<span class="st">&#39;images&#39;</span>],</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>                            data_dict[<span class="st">&#39;test&#39;</span>][<span class="st">&#39;labels&#39;</span>],</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>                            transform<span class="op">=</span>transform)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>holdout_dataset <span class="op">=</span> LetterDataset(data_dict[<span class="st">&#39;holdout&#39;</span>][<span class="st">&#39;images&#39;</span>],</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>                                data_dict[<span class="st">&#39;holdout&#39;</span>][<span class="st">&#39;labels&#39;</span>],</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>                                transform<span class="op">=</span>transform)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataloaders</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_dataset, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(val_dataset, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>holdout_loader <span class="op">=</span> DataLoader(holdout_dataset, batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and training components</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">&#39;cuda&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BasicCNN().to(device)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>train_model(model, train_loader, val_loader, device)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on holdout set</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> holdout_loader:</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        images, labels <span class="op">=</span> images.to(device), labels.to(device)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(images)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> outputs.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> predicted.eq(labels).<span class="bu">sum</span>().item()</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>holdout_acc <span class="op">=</span> <span class="fl">100.</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Holdout Accuracy: </span><span class="sc">{</span>holdout_acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span></code></pre></div>
</div>
<section id="target-accuracy-80-on-holdout-set" class="cell markdown"
id="ecHzhz5Mqzrx">
<h3>Target Accuracy: 80% on Holdout Set</h3>
</section>
<div class="cell markdown" id="pH0iEqP6qzrx">
<p>Unfortunately, despite having the text, you still can't read it. It
appears to be encoded with some kind of cipher. If only there were
seq2seq models that you maybe could use to decode it...</p>
</div>
<script defer src="https://static.cloudflareinsights.com/beacon.min.js/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon='{"rayId":"928fd745ee8110bc","version":"2025.3.0","r":1,"token":"0357a45f23a943f08700f7f9af191ee6","serverTiming":{"name":{"cfExtPri":true,"cfL4":true,"cfSpeedBrain":true,"cfCacheStatus":true}}}' crossorigin="anonymous"></script>
</body>
</html>
